{"cells":[{"cell_type":"markdown","metadata":{"id":"aaCqxk7RoyQD"},"source":["#Setup\n","Folgende Befehle müssen vor dem Verwenden ausgeführt werden. Die aktuelle Implementation verwendet ein lokales Google Drive, weshalb Dateipfade vor dem verwenden angepasst werden müssen."]},{"cell_type":"markdown","metadata":{"id":"NEsBk2bepC8D"},"source":["Check GPU, CPU and CUDA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rAvoko7PofIA"},"outputs":[],"source":["import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\"\n","# Check GPU and CUDA\n","!nvidia-smi\n","!nvcc --version"]},{"cell_type":"markdown","metadata":{"id":"fTfBAqwIpP3b"},"source":["Mount your google drive containing all files you want to use later on. Allows easy load and saving Operations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FsR68gIPpbt1"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"xiMtg1cApfbr"},"source":["Install all needed requirements and adjust system path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lA2xEOAUpl8T"},"outputs":[],"source":["!pip install einops ninja gdown\n","import sys\n","sys.path.append('/content/drive/My Drive/Bachelorarbeit/bachelorarbeit/stylegan3')"]},{"cell_type":"markdown","metadata":{"id":"y6878D5AppgK"},"source":["##Imports\n","Alle verwendeten Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mM1O_qjKpvur"},"outputs":[],"source":["import os\n","from skimage import io as ios\n","\n","import io\n","import re\n","from typing import List, Optional, Tuple, Union\n","\n","import click\n","import dnnlib\n","import numpy as np\n","import PIL.Image\n","import torch\n","import torchvision\n","import legacy\n","import time\n","from typing import BinaryIO, Union, IO\n","from training.networks_stylegan2 import Generator\n","from matplotlib import pyplot as plt\n","import matplotlib\n","import cv2\n","import IPython.display\n","import dlib\n","import glob\n","\n","import requests\n","import html\n","import hashlib\n","import PIL.ImageFile\n","import scipy.ndimage\n","import threading\n","import queue\n","import time\n","import json\n","import uuid\n","import argparse\n","import itertools\n","import shutil\n","from collections import OrderedDict, defaultdict\n","import collections\n","\n","from pickle import NONE\n","\n","import math\n","import torch.nn.functional as F\n","\n","import pickle\n","\n","\n","#TSNE Imports\n","from sklearn.manifold import TSNE\n","from keras.datasets import mnist\n","from sklearn.datasets import load_iris\n","from numpy import reshape\n","import seaborn as sns\n","import pandas as pd  \n","import random\n","import pylab\n","from mpl_toolkits.mplot3d import Axes3D\n","from matplotlib.colors import ListedColormap"]},{"cell_type":"markdown","metadata":{"id":"3ax2pD9VqO2i"},"source":["## Utility functions\n","\n","Helferfunktionen"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NGpSKtQNqVW7"},"outputs":[],"source":["DEVICE = 'cuda'\n","\n","\n","\n","def generate_z_from_seed(seed: int, num_samples: int, truncation_psi: float, device: torch.device, dim: int) -> torch.Tensor:\n","    '''Generate a latent vector Z using the given random seed. Default truncation_psi = 0.7'''\n","    torch.manual_seed(seed)\n","    z = torch.randn(num_samples, dim, device=device).to(device)\n","    z = z * truncation_psi\n","    return z\n","\n","def open_img_tensor(img_tensor_path: Union[str, os.PathLike, BinaryIO, IO[bytes]]) -> torch.Tensor:\n","    '''Loads the image at img_tensor_path and returns the corresponding torch Tensor'''\n","    img = np.asarray(PIL.Image.open(img_tensor_path))\n","\n","\n","    img_tensor = torch.Tensor(img)\n","    img_tensor = img_tensor.view(1, 256, 256, 3).to(DEVICE)\n","    img_tensor = (img_tensor - 128) / 127.5\n","    img_tensor = img_tensor.permute(0, 3, 1, 2)\n","\n","    return img_tensor\n","\n","\n","def plot_generator_img(img_tensor: torch.Tensor, title: str) -> None:\n","    ''' Visualize a single image tensors'''\n","    img = (img_tensor.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","    img = img[0].cpu().numpy()\n","\n","    plt.title(title)\n","    plt.axis('off')\n","    plt.imshow(img)\n","    plt.show()\n","\n","def plot_multiple_generator_img(img_tensors: torch.Tensor, title: str, start_seed, count, col) -> None:\n","    ''' Visualize multiple image tensors'''\n","    img = (img_tensors.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","    img = img.cpu().numpy()\n","\n","\n","    col = col\n","    row = img.shape[0] // col + 1\n","    fig = plt.figure(figsize=(40,40))\n","\n","    ax = []\n","\n","    for i in range(col*row):\n","      if (i < count):\n","        cur_img = img[i]\n","\n","      else:\n","        cur_img = np.zeros((256,256))\n","\n","      ax.append( fig.add_subplot(row, col, i+1) )\n","      ax[-1].set_title(f'Seed {start_seed+i}')  # set title\n","      plt.imshow(cur_img)\n","\n","    plt.title(title)\n","    plt.show()\n","\n","\n","\n","def resize_img(img_path: Union[str, os.PathLike, BinaryIO, IO[bytes]], size:int) -> None:\n","    '''Loads the Image at img_path, resizes it to sizexsize and save it at img_path'''\n","    img = PIL.Image.open(img_path)\n","    img = img.resize((size, size))\n","    img.save(img_path)\n","\n","def gen_rand_z(gen: Generator) -> torch.Tensor:\n","    '''Generates a random latent Code z with fitting size'''\n","    return torch.from_numpy(np.random.randn(1, gen.z_dim)).to(DEVICE)\n","\n","def save_generator_img(img_tensor: torch.Tensor, path: Union[str, os.PathLike, BinaryIO, IO[bytes]]) -> None:\n","    '''Save an image generated by the stylegan generator'''\n","    img = (img_tensor.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","    img = img[0].cpu().numpy()\n","\n","    PIL.Image.fromarray(img, 'RGB').save(path)\n","\n","def init_generator(PKL):\n","  '''Initialize the generator for given network pickel '''\n","  #print('Loading networks from \"%s\"...' % NETWORK_PKL)\n","  device = torch.device('cuda')\n","  with dnnlib.util.open_url(NETWORK_PKL) as f:\n","    G = legacy.load_network_pkl(f)['G_ema'].to(device)  # type: ignore\n","  return G\n","\n","\n","def load_tensor(path):\n","  '''Loads the torch tensor at given path '''\n","  w = torch.load(path)\n","  return(w)\n","\n","def save_tensor(tensor, path):\n","  '''Saves the given torch tensor to given path '''\n","  torch.save(tensor, path)\n","\n","def imshow(images, col, viz_size=256):\n","  \"\"\"Plots Images in one Figure \"\"\"\n","  #Get Number and shapes\n","\n","  num, height, width, channels = images.shape\n","  assert num % col == 0\n","  row = num // col\n","  \n","\n","  fused_image = np.zeros((viz_size * row, viz_size * col, channels), dtype=np.uint8)\n","\n","  for idx, image in enumerate(images):\n","    i, j = divmod(idx, col)\n","    y = i * viz_size\n","    x = j * viz_size\n","    if height != viz_size or width != viz_size:\n","      image = cv2.resize(image, (viz_size, viz_size))\n","    fused_image[y:y + viz_size, x:x + viz_size] = image\n","\n","  fused_image = np.asarray(fused_image, dtype=np.uint8)\n","  data = io.BytesIO()\n","  PIL.Image.fromarray(fused_image).save(data, 'jpeg')\n","  im_data = data.getvalue()\n","  disp = IPython.display.display(IPython.display.Image(im_data))\n","  return disp\n","\n","\n","\n","\n","def prepare_image(img_path, dim):\n","  '''Prepares an 256x256 image to for projection'''\n","\n","  target_images = open_img_tensor(img_path).to('cpu')\n","\n","  target_images = np.asarray(target_images, dtype='float32')\n","\n","  \n","  #target_images = (target_images + 1) * (255 / 2)\n","  plot_generator_img(torch.from_numpy(target_images), \"OG\")\n"," \n","\n","  sh = target_images.shape\n","  print(sh)\n","  if sh[2] > dim:\n","      factor = sh[2] // dim\n","      target_images = np.reshape(target_images, [-1, sh[1], sh[2] // factor, factor, sh[3] // factor, factor]).mean((3, 5))\n","  plot_generator_img(torch.from_numpy(target_images), \"reshaped\")\n","\n","  target_images = torch.from_numpy(target_images).to('cuda')\n","\n","  return(target_images)"]},{"cell_type":"markdown","metadata":{"id":"xvnGnAZAqxj7"},"source":["## VGG Percptual loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fTZ0PCbHq4Wj"},"outputs":[],"source":["class VGGPerceptualLoss(torch.nn.Module):\n","    '''Pretrained VGG-Model for VGG Loss, source https://gist.github.com/alper111/8233cdb0414b4cb5853f2f730ab95a49'''\n","    def __init__(self, resize=True):\n","        super(VGGPerceptualLoss, self).__init__()\n","        blocks = []\n","        blocks.append(torchvision.models.vgg16(pretrained=True).features[:4].eval())\n","        blocks.append(torchvision.models.vgg16(pretrained=True).features[4:9].eval())\n","        blocks.append(torchvision.models.vgg16(pretrained=True).features[9:16].eval())\n","        blocks.append(torchvision.models.vgg16(pretrained=True).features[16:23].eval())\n","        for bl in blocks:\n","            for p in bl.parameters():\n","                p.requires_grad = False\n","        self.blocks = torch.nn.ModuleList(blocks)\n","        self.transform = torch.nn.functional.interpolate\n","        self.resize = resize\n","        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n","        self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n","\n","    def forward(self, input, target, feature_layers=[0, 1, 2, 3], style_layers=[]):\n","\n","        if input.shape[1] != 3:\n","            input = input.repeat(1, 3, 1, 1)\n","            target = target.repeat(1, 3, 1, 1)\n","        input = (input - self.mean) / self.std\n","        target = (target - self.mean) / self.std\n","        if self.resize:\n","            input = self.transform(input, mode='bilinear', size=(224, 224), align_corners=False)\n","            target = self.transform(target, mode='bilinear', size=(224, 224), align_corners=False)\n","        loss = 0.0\n","        x = input\n","        y = target\n","        for i, block in enumerate(self.blocks):\n","            x = block(x)\n","            y = block(y)\n","            if i in feature_layers:\n","                loss += torch.nn.functional.l1_loss(x, y)\n","            if i in style_layers:\n","                act_x = x.reshape(x.shape[0], x.shape[1], -1)\n","                act_y = y.reshape(y.shape[0], y.shape[1], -1)\n","                gram_x = act_x @ act_x.permute(0, 2, 1)\n","                gram_y = act_y @ act_y.permute(0, 2, 1)\n","                loss += torch.nn.functional.l1_loss(gram_x, gram_y)\n","        return loss\n"]},{"cell_type":"markdown","metadata":{"id":"z3in-xQ4p0cz"},"source":["# Morphing\n","Our first application is morphing between to randomly generated images. We will morph in z-space and w-space and compare the paths taken by linear morphing."]},{"cell_type":"markdown","metadata":{"id":"QS4HG4Gd1SVY"},"source":["##Morphing in z-space\n","We start by generating two z-Tensors and morphing them in z-space and evaluating to effect on the generated image."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yDegOh-j1Poq"},"outputs":[],"source":["NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","PATH = \"/content/drive/My Drive/Bachelorarbeit/ausarbeitung/gfx/morphing/z-space\"\n","DEVICE = 'cuda'\n","def morph(zs, steps):\n","  '''Computes the latent Code for every step in the interpolation'''\n","    morphed_zs = []\n","    for i in range(len(zs)-1):\n","        for index in range(steps):\n","            t = index/float(steps)\n","            morphed_zs.append(zs[i+1]*t + zs[i]*(1-t))\n","    return morphed_zs\n","\n","def z_morph(seed1, seed2, steps):\n","  '''linear morph between the latent code given by seed1 and the latenten code given by seed2 in number of steps steps.'''\n","  truncation_psi = 0.7\n","  noise_mode = 'const'\n","\n","  #Init generator\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=DEVICE)\n","\n","  #get latent codes\n","  z1 = np.random.RandomState(seed1).randn(1, G.z_dim)  \n","  z2 = np.random.RandomState(seed2).randn(1, G.z_dim)\n","\n","\n","  #get latente codes for every interpolation step\n","  morphed_zs = morph([z1,z2], steps)\n","  \n","\n","  #initliaize loss function and target image \n","  loss = torch.nn.MSELoss()\n","  target = G(torch.from_numpy(z2).to(DEVICE), label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","\n","\n","  error = []\n","  \n","  #Loop through every code, compute loss to target image and save the generated image and corresponding tensor.\n","  for idx, z in enumerate(morphed_zs):\n","    z = torch.from_numpy(z).to(DEVICE)\n","    img = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","    error.append(loss(target, img).item())\n","\n","\n","    if(idx % 10 == 0):\n","      plot_generator_img(img, f\"image{idx}\")\n","    save_generator_img(img,  f'{PATH}/frame-{idx:04d}.png')\n","    save_tensor(z, PATH + f\"/tensor{idx}.pt\")\n","\n","  #Plot loss\n","  matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n","  plt.plot(error)\n","  plt.xlabel(\"Interpolations Schritt\")\n","  plt.ylabel(\"MSE-Loss\")\n","  plt.title(\"MSE-Loss im Z-Raum\")\n","  plt.show()\n","\n","  print(error)\n","\n","#Function call\n","z_morph(337, 338, 50)\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"znATiLnpg3Gf"},"source":["Covert frames into a video"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"efT76zkyg2XI"},"outputs":[],"source":["!ffmpeg -i /content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/morphing/z-space/frame-%04d.png -r 24 -vcodec libx264 -pix_fmt yuv420p /content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/morphing/z-space/morph.mp4"]},{"cell_type":"markdown","metadata":{"id":"WBvokzVi8dbi"},"source":["## Morphing in w-space\n","We start by generating two z-Tensors of our start and end images. We map the tensors to the belonging w-tensors and morph them in w-space and evaluating to effect on the generated image."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JNHa5jh88uqf"},"outputs":[],"source":["NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","PATH = \"/content/drive/My Drive/Bachelorarbeit/ausarbeitung/gfx/morphing/w-space\"\n","DEVICE = 'cuda'\n","\n","def morph(zs, steps):\n","    morphed_zs = []\n","    for i in range(len(zs)-1):\n","        for index in range(steps):\n","            t = index/float(steps)\n","            morphed_zs.append(zs[i+1]*t + zs[i]*(1-t))\n","    return morphed_zs\n","\n","def w_morph(seed1, seed2, steps):\n","  '''Same as z_morph but map latente codes z to their intermediate latent codes w and perform the interpolation in W Space'''\n","  truncation_psi = 0.7\n","  noise_mode = 'const'\n","\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=DEVICE)\n","\n","  #generate z codes and map them to their corresponding w codes\n","  z1 = torch.from_numpy(np.random.RandomState(seed1).randn(1, G.z_dim)).to(DEVICE)  \n","  w1 = G.mapping(z1, label, truncation_psi=truncation_psi, truncation_cutoff=8)\n","\n","  z2 = torch.from_numpy(np.random.RandomState(seed2).randn(1, G.z_dim)).to(DEVICE)\n","  w2 = G.mapping(z2, label, truncation_psi=truncation_psi, truncation_cutoff=8)\n","\n","  loss = torch.nn.MSELoss()\n","  target = G.synthesis(w2, noise_mode=noise_mode, force_fp32=False)\n","  error = []\n","\n","  morphed_ws = morph([w1,w2], steps)\n","\n","  for idx, w in enumerate(morphed_ws):\n","\n","    print(w.shape)\n","    img = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","    print(img.shape)\n","    error.append(loss(target, img).item())\n","\n","    if(idx % 10 == 0):\n","      plot_generator_img(img, f\"image{idx:04d}\")\n","    save_generator_img(img, f'{PATH}/frame-{idx:04d}.png')\n","    save_tensor(w, PATH + f\"/tensor{idx:04d}.pt\")\n","\n","  plt.plot(error)\n","  plt.xlabel(\"Interpolations Schritt\")\n","  plt.ylabel(\"MSE-Loss\")\n","  plt.title(\"MSE-Loss im W-Raum\")\n","  plt.show()\n","\n","\n","\n","w_morph(337, 338, 50)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MPmUr0MXb1Om"},"source":["Convert Pictures into a video"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rik4vRXTb7hE"},"outputs":[],"source":["!ffmpeg -i /content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/morphing/w-space/frame-%04d.png -r 24 -vcodec libx264 -pix_fmt yuv420p /content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/morphing/w-space/morph.mp4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FeeHVZZ_eYWg"},"outputs":[],"source":["!ffmpeg -i /content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/presentation/morphs/morph2/frame-%04d.png -r 24 -vcodec libx264 -pix_fmt yuv420p /content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/presentation/morphs/morph2/morph.mp4"]},{"cell_type":"markdown","metadata":{"id":"-oayOqazrPtn"},"source":["##Boilerplate\n","Only used for graphics, otherwise no real usage"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XkyOg1QMrTgM"},"outputs":[],"source":["NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","PATH = \"/content/drive/My Drive/Bachelorarbeit/ausarbeitung/gfx/morphing/z-space\"\n","DEVICE = 'cuda'\n","\n","def morph(zs, steps):\n","    morphed_zs = []\n","    for i in range(len(zs)-1):\n","        for index in range(steps):\n","            t = index/float(steps)\n","            morphed_zs.append(zs[i+1]*t + zs[i]*(1-t))\n","    return morphed_zs\n","\n","def z_morph(seed1, seed2, steps):\n","  truncation_psi = 0.7\n","  noise_mode = 'const'\n","\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=DEVICE)\n","\n","  z1 = np.random.RandomState(seed1).randn(1, G.z_dim)  \n","  z2 = np.random.RandomState(seed2).randn(1, G.z_dim)\n","\n","  w1 = G.mapping(torch.from_numpy(z1).to(DEVICE) , label, truncation_psi=truncation_psi, truncation_cutoff=8)\n","  w2 = G.mapping(torch.from_numpy(z2).to(DEVICE) , label, truncation_psi=truncation_psi, truncation_cutoff=8)\n","\n","\n","  morphed_zs = morph([z1,z2], steps)\n","  morphed_ws = morph([w1,w2], steps)\n","  \n","\n","  loss = torch.nn.MSELoss()\n","  target = G(torch.from_numpy(z2).to(DEVICE), label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","\n","  error_z = []\n","  error_w = []\n","\n","  for idx, z in enumerate(morphed_zs):\n","    z = torch.from_numpy(z).to(DEVICE)\n","    img = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","    error_z.append(loss(target, img).item())\n","\n","  for idx, w in enumerate(morphed_ws):\n","    img = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","    error_w.append(loss(target, img).item())\n","\n","\n","  figure, axis = plt.subplots(1, 2)\n","  axis[0].plot(error_z)\n","  axis[0].set_xlabel(\"Interpolations Schritt\")\n","  axis[0].set_ylabel(\"MSE-Loss\")\n","  axis[0].set_title(\"MSE-Loss in Z-Raum\")\n","\n","  axis[1].plot(error_w)\n","  axis[1].set_xlabel(\"Interpolations Schritt\")\n","  axis[1].set_ylabel(\"MSE-Loss\")\n","  axis[1].set_title(\"MSE-Loss in W-Raum\")\n","\n","  plt.show()\n","\n","\n","z_morph(335, 336, 50)\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jcLPSOdU9fVh"},"source":["# Latent Arithmetics"]},{"cell_type":"markdown","metadata":{"id":"MPpb5Nvl9oC0"},"source":["## Distance calculations in stylegans latent spaces"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HDjJTBSC-a4-"},"outputs":[],"source":["NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","PATH = \"/content/drive/My Drive/Bachelorarbeit/bachelorarbeit/morphing/w-space\"\n","DEVICE = 'cuda'\n","\n","\n","def morph(zs, steps):\n","  morphed_zs = []\n","  for i in range(len(zs)-1):\n","      for index in range(steps):\n","          t = index/float(steps)\n","          morphed_zs.append(zs[i+1]*t + zs[i]*(1-t))\n","  return morphed_zs\n","\n","def z_distance(seed1, seed2, steps):\n","  '''Distances between Images when moving in z latent space'''\n","\n","  truncation_psi = 0.7\n","  noise_mode = 'const'\n","\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=DEVICE)\n","\n","  z1 = np.random.RandomState(seed1).randn(1, G.z_dim)  \n","\n","  z2 = np.random.RandomState(seed2).randn(1, G.z_dim)\n","\n","  morphed_zs = morph([z1,z2], steps)\n","\n","\n","  loss = torch.nn.MSELoss()\n","  start = G(torch.from_numpy(z1).to(DEVICE), label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","\n","  error = []\n","  distance = []\n","  \n","  z1_t = torch.from_numpy(z1).to(DEVICE)\n","\n","  for z in morphed_zs:\n","    z = torch.from_numpy(z).to(DEVICE)\n","    residual = z1_t - z\n","    length = torch.linalg.norm(residual)\n","    img = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","    error.append(loss(start, img).item())\n","    distance.append(length.item())\n","\n","  plt.plot(distance, error)\n","  plt.xlabel(\"distance\")\n","  plt.ylabel(\"MSE error\")\n","  plt.title(\"MSE error for different distances\")\n","  plt.show()\n","\n","def w_distance(seed1, seed2, steps):\n","  '''Distances between Images when moving in w latent space'''\n","  truncation_psi = 0.7\n","  noise_mode = 'const'\n","\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=DEVICE)\n","\n","  z1 = torch.from_numpy(np.random.RandomState(seed1).randn(1, G.z_dim)).to(DEVICE)  \n","  w1 = G.mapping(z1, label, truncation_psi=truncation_psi, truncation_cutoff=8)\n","\n","  z2 = torch.from_numpy(np.random.RandomState(seed2).randn(1, G.z_dim)).to(DEVICE)\n","  w2 = G.mapping(z2, label, truncation_psi=truncation_psi, truncation_cutoff=8)\n","\n","  loss = torch.nn.MSELoss()\n","  start = G.synthesis(w1, noise_mode=noise_mode, force_fp32=False)\n","  error = []\n","  distance = []\n","\n","  morphed_ws = morph([w1,w2], steps)\n","\n","  for w in morphed_ws:\n","    residual = w1 - w\n","    length = torch.linalg.norm(residual)\n","    img = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","    error.append(loss(start, img).item())\n","    distance.append(length.item())\n","\n","  plt.plot(distance, error)\n","  plt.xlabel(\"distance\")\n","  plt.ylabel(\"MSE error\")\n","  plt.title(\"MSE error for different distances\")\n","  plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9H8IGMTDA5dT"},"outputs":[],"source":["z_distance(200,201,100)\n","w_distance(200,201,100)"]},{"cell_type":"markdown","metadata":{"id":"TfpdaqZ69lYr"},"source":["##Finding desired Features "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H9Uw_c3w9qU-"},"outputs":[],"source":["NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","DEVICE = 'cuda'\n","\n","def display_seeds(start_seed, count):\n","  '''Displays the images corresponding to the latent codes generated by seeds start_seed until seed start_seed+count.'''\n","\n","  truncation_psi = 0.7\n","  noise_mode = 'const'\n","\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=DEVICE)\n","\n","  zs = [torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE)  for seed in range(start_seed, start_seed + count)]\n","\n","  zs = torch.cat([z for z in zs]).to(DEVICE)\n","\n","  img_tensor = G(zs, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","\n","  plot_multiple_generator_img(img_tensor, f'Seeds {start_seed} to {start_seed+count}', start_seed, count, 6 )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-3r0Wbz_-6x9"},"outputs":[],"source":["display_seeds(140, 70)\n","\n","\n","#Hand classified Arrays for certain attributes.\n","men = [0,4,7,8,12,13,14,15,16,18,21,27,28,29,33,34,35,37,40,43,44,45,46,47,48,49,51,52,58,59,65,66,71,72,73,74,75,78,80,82,83,100,103,105,107,108,109,115,123,204,205,206,207,208,324,326,336]\n","women = [6,9,17,22,23,24,25,26,30,31,32,39,41,42,50,53,55,56,57,60,62,64,67,68,70,76,77,79,101,102,104,106,110,111,112,113,114,116,117,118,119,120,121,122,124,127,128,200,201,202,210,211,213,214,222,221,222,223,304,305, 309,346,347,360,361,362,363,364]\n","men2= [540,550,551,555,556,559,568,614,612,623,624,628,635,637,645,648,651,655,662,672,680,681,708,711,712,717,718]\n","women2 = [547,548,549,553,554,557,560,561,563,564,565,569,625,631,632,641,642,650,653,665,682,683,685,688,690,692,694,700,701,702,703,710,713,714,715,720,722,723,728,729]\n","\n","\n","\n","child = [203,209,215,216,217,226,230,322,325,345,400,401,409,415,422,424,425,466,468,479,495,501,499,501,514,516,542,546,573,582,583,592,596,597,630,644,639,640,646,656,677]\n","child2 = [686,689,691,693,696,704,709,719,724,737]\n","teen = [205,225,245,302,303,313,336,349,344,356]\n","adult = [200,201,202,204,206,207,208,227,228,232,238,240,241,300,402,404,403,405,406,407,410,412,413,414,416,417,418,419,420,421,423,427,428,429,439,431,432,433,434,345]\n","adult2 = [547,548,549,553,554,557,560,561,563,564,540,550,551,555,556,559,568,614,612,623]\n","old = [218,222,224,233,236,249,261,265,309,311,316,334,367,368,411,426,474,540,574,589,638,645,667,655,673,707,717,747,727]\n","\n","glasses_men =[206, 311, 334, 336, 339, 348, 352, 355, 356, 373, 374, 376, 775, 758,776,780,788,790,796,821,831,840,843,846,878,891,915, 916,917,944, 962, 965, 970, 975, 988,990,993,1006,517]\n","glasses_women =[255, 295, 291, 286, 309, 347, 384,683,770,798,851,874,900,935,977,1001,513,195]\n","glasses = glasses_men + glasses_women\n","smiling_men = [256,259, 273, 275, 300, 316, 320, 324]\n","smiling_women = [210, 214, 260, 272, 274, 276, 302, 309, 313, 338, 347,346, 350]\n","neutral_men = [218,257,258, 261, 264, 265, 307, 318, 319]\n","neutral_women = [211, 310, 312, 334, 351, 359]\n","smiling_child = [209, 216, 366]\n","neutral_child = [322,325,328, 341, 345, 356, 357]\n","beard = [218, 308, 318, 373,970,1013,16,123,181]\n","lipstick = [222, 225, 303, 305, 310, 763,888,907,1017,1029,547,198,192]\n","darker_skin_women = [272, 290, 384, 504,121,128]\n","darker_skin_men = [229, 271, 348, 349, 375, 399, 386,804,807,824,860,80,82]\n"]},{"cell_type":"markdown","source":["###Geschlecht\n","Visualization of latenten Codes for different genders in w space"],"metadata":{"id":"j4jQAKYgVEC3"}},{"cell_type":"code","source":["NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","\n","device = 'cuda'\n","truncation_psi = 0.7\n","noise_mode = 'const'\n","\n","men = [0,4,7,8,12,13,14,15,16,18,21,27,28,29,33,34,35,37,40,43,44,45,46,47,48,49,51,52,58,59,65,66,71,72,73,74,75,78,80,82,83,100,103,105,107,108,109,115,123,204,205,206,207,208,324,326,336]\n","women = [6,9,17,22,23,24,25,26,30,31,32,39,41,42,50,53,55,56,57,60,62,64,67,68,70,76,77,79,101,102,104,106,110,111,112,113,114,116,117,118,119,120,121,122,124,127,128,200,201,202,210,211,213,214,222,221,222,223,304,305, 309,346,347,360,361,362,363,364]\n","men2= [540,550,551,555,556,559,568,614,612,623,624,628,635,637,645,648,651,655,662,672,680,681,708,711,712,717,718]\n","women2 = [547,548,549,553,554,557,560,561,563,564,565,569,625,631,632,641,642,650,653,665,682,683,685,688,690,692,694,700,701,702,703,710,713,714,715,720,722,723,728,729]\n","\n","men = men + men2\n","women = women + women2\n","\n","\n","child = [203,209,215,216,217,226,230,322,325,345,400,401,409,415,422,424,425,466,468,479,495,501,499,501,514,516,542,546,573,582,583,592,596,597,630,644,639,640,646,656,677]\n","child2 = [686,689,691,693,696,704,709,719,724,737,]\n","teen = [205,225,245,302,303,313,336,349,344,356]\n","adult = [200,201,202,204,206,207,208,227,228,232,238,240,241,300,402,404,403,405,406,407,410,412,413,414,416,417,418,419,420,421,423,427,428,429,439,431,432,433,434,345]\n","adult2 = [547,548,549,553,554,557,560,561,563,564,540,550,551,555,556,559,568,614,612,623]\n","old = [218,222,224,233,236,249,261,265,309,311,316,334,367,368,411,426,474,540,574,589,638,645,667,655,673,707,717,747,727]\n","\n","G = init_generator(NETWORK_PKL)\n","label = torch.zeros([1, G.c_dim], device=device)\n","\n","size = 300\n","\n","#Initalize latent codes for hand classified images\n","w_men = np.array([G.mapping(torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE), label, truncation_psi=truncation_psi).cpu().numpy().astype(np.float32) for seed in men])\n","w_women = np.array([G.mapping(torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE), label, truncation_psi=truncation_psi).cpu().numpy().astype(np.float32) for seed in women])\n","\n","\n","w_men = reshape(w_men, [w_men.shape[0], w_men.shape[1]*w_men.shape[2]*w_men.shape[3]])\n","w_women = reshape(w_women, [w_women.shape[0], w_women.shape[1]*w_women.shape[2]*w_women.shape[3]])\n","\n","\n","#initalize t-SNE, labels and colors\n","color1 = [\"Männer\" for  i in range(len(men))]\n","color2 = [\"Frauen\" for  i in range(len(women))]\n","\n","color =  color1 + color2\n","ws = np.concatenate((w_men, w_women))\n","imgs = np.concatenate((img_men, img_women))\n","\n","\n","tsne = TSNE(n_components=2, verbose=1, random_state=123, perplexity=40)\n","embed = tsne.fit_transform(ws)\n","\n","\n","df = pd.DataFrame()\n","df[\"y\"] = color\n","df[\"comp-1\"] = embed[:,0]\n","df[\"comp-2\"] = embed[:,1]\n","\n","\n","\n","\n","# hue=df.y.tolist()\n","sns.scatterplot(x=\"comp-1\", y=\"comp-2\",hue=df.y.tolist(),\n","                palette=sns.color_palette(\"hls\", 2),\n","                data=df).set(title=\"Geschlecht W Raum\") \n","\n"],"metadata":{"id":"PmbVIFcYVFtH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tInzmj6e9VbF"},"source":["###Geschlecht Z-Space\n","Visualization of latenten Codes for different genders in z space"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TmxaP_Ph9f_T"},"outputs":[],"source":["NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","\n","device = 'cuda'\n","truncation_psi = 0.7\n","noise_mode = 'const'\n","\n","men = [0,4,7,8,12,13,14,15,16,18,21,27,28,29,33,34,35,37,40,43,44,45,46,47,48,49,51,52,58,59,65,66,71,72,73,74,75,78,80,82,83,100,103,105,107,108,109,115,123,204,205,206,207,208,324,326,336]\n","women = [6,9,17,22,23,24,25,26,30,31,32,39,41,42,50,53,55,56,57,60,62,64,67,68,70,76,77,79,101,102,104,106,110,111,112,113,114,116,117,118,119,120,121,122,124,127,128,200,201,202,210,211,213,214,222,221,222,223,304,305, 309,346,347,360,361,362,363,364]\n","men2= [540,550,551,555,556,559,568,614,612,623,624,628,635,637,645,648,651,655,662,672,680,681,708,711,712,717,718]\n","women2 = [547,548,549,553,554,557,560,561,563,564,565,569,625,631,632,641,642,650,653,665,682,683,685,688,690,692,694,700,701,702,703,710,713,714,715,720,722,723,728,729]\n","\n","men = men + men2\n","women = women + women2\n","\n","\n","child = [203,209,215,216,217,226,230,322,325,345,400,401,409,415,422,424,425,466,468,479,495,501,499,501,514,516,542,546,573,582,583,592,596,597,630,644,639,640,646,656,677]\n","child2 = [686,689,691,693,696,704,709,719,724,737,]\n","teen = [205,225,245,302,303,313,336,349,344,356]\n","adult = [200,201,202,204,206,207,208,227,228,232,238,240,241,300,402,404,403,405,406,407,410,412,413,414,416,417,418,419,420,421,423,427,428,429,439,431,432,433,434,345]\n","adult2 = [547,548,549,553,554,557,560,561,563,564,540,550,551,555,556,559,568,614,612,623]\n","old = [218,222,224,233,236,249,261,265,309,311,316,334,367,368,411,426,474,540,574,589,638,645,667,655,673,707,717,747,727]\n","\n","G = init_generator(NETWORK_PKL)\n","label = torch.zeros([1, G.c_dim], device=device)\n","\n","size = 300\n","\n","#generate z codes of hand classified images\n","z_men = np.array([np.random.RandomState(seed).randn(1, G.z_dim) for seed in men])\n","z_women = np.array([np.random.RandomState(seed).randn(1, G.z_dim) for seed in women])\n","\n","\n","\n","\n","z_men = reshape(z_men, [z_men.shape[0], z_men.shape[1]*z_men.shape[2]])\n","z_women = reshape(z_women, [z_women.shape[0], z_women.shape[1]*z_women.shape[2]])\n","\n","\n","#initalize t-SNE, labels and colors\n","color1 = [\"Männer\" for  i in range(len(men))]\n","color2 = [\"Frauen\" for  i in range(len(women))]\n","\n","color =  color1 + color2\n","zs = np.concatenate((z_men, z_women))\n","\n","tsne = TSNE(n_components=2, verbose=1, random_state=123, perplexity=30)\n","embed = tsne.fit_transform(zs)\n","\n","#use pandas and seaborn for plotting\n","df = pd.DataFrame()\n","df[\"y\"] = color\n","df[\"comp-1\"] = embed[:,0]\n","df[\"comp-2\"] = embed[:,1]\n","\n","\n","# hue=df.y.tolist()\n","sns.scatterplot(x=\"comp-1\", y=\"comp-2\",hue=df.y.tolist(),\n","                palette=sns.color_palette(\"hls\", 2),\n","                data=df).set(title=\"Geschlecht Z Raum\") "]},{"cell_type":"markdown","metadata":{"id":"eS8KlwJ0-3jk"},"source":["###Alter\n","Visualization of latenten Codes for different ages in w space"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cA1s1MhN-5F0"},"outputs":[],"source":["##Visualisation\n","\n","NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","\n","device = 'cuda'\n","truncation_psi = 0.7\n","noise_mode = 'const'\n","\n","men = [0,4,7,8,12,13,14,15,16,18,21,27,28,29,33,34,35,37,40,43,44,45,46,47,48,49,51,52,58,59,65,66,71,72,73,74,75,78,80,82,83,100,103,105,107,108,109,115,123,204,205,206,207,208,324,326,336]\n","women = [6,9,17,22,23,24,25,26,30,31,32,39,41,42,50,53,55,56,57,60,62,64,67,68,70,76,77,79,101,102,104,106,110,111,112,113,114,116,117,118,119,120,121,122,124,127,128,200,201,202,210,211,213,214,222,221,222,223,304,305, 309,346,347,360,361,362,363,364]\n","men2= [540,550,551,555,556,559,568,614,612,623,624,628,635,637,645,648,651,655,662,672,680,681,708,711,712,717,718]\n","women2 = [547,548,549,553,554,557,560,561,563,564,565,569,625,631,632,641,642,650,653,665,682,683,685,688,690,692,694,700,701,702,703,710,713,714,715,720,722,723,728,729]\n","\n","men = men + men2\n","women = women + women2\n","\n","\n","child = [203,209,215,216,217,226,230,322,325,345,400,401,409,415,422,424,425,466,468,479,495,501,499,501,514,516,542,546,573,582,583,592,596,597,630,644,639,640,646,656,677]\n","child2 = [686,689,691,693,696,704,709,719,724,737,]\n","teen = [205,225,245,302,303,313,336,349,344,356]\n","adult = [200,201,202,204,206,207,208,227,228,232,238,240,241,300,402,404,403,405,406,407,410,412,413,414,416,417,418,419,420,421,423,427,428,429,439,431,432,433,434,345]\n","adult2 = [547,548,549,553,554,557,560,561,563,564,540,550,551,555,556,559,568,614,612,623]\n","old = [218,222,224,233,236,249,261,265,309,311,316,334,367,368,411,426,474,540,574,589,638,645,667,655,673,707,717,747,727]\n","\n","#These latente Codes work best to show the seperation in w space\n","child = child + child2\n","adult = old \n","G = init_generator(NETWORK_PKL)\n","label = torch.zeros([1, G.c_dim], device=device)\n","\n","size = 300\n","\n","w_child = np.array([G.mapping(torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE), label, truncation_psi=truncation_psi).cpu().numpy().astype(np.float32) for seed in child])\n","w_teen = np.array([G.mapping(torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE), label, truncation_psi=truncation_psi).cpu().numpy().astype(np.float32) for seed in teen])\n","w_adult = np.array([G.mapping(torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE), label, truncation_psi=truncation_psi).cpu().numpy().astype(np.float32) for seed in adult])\n","w_old = np.array([G.mapping(torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE), label, truncation_psi=truncation_psi).cpu().numpy().astype(np.float32) for seed in old])\n","\n","\n","\n","w_child = reshape(w_child, [w_child.shape[0], w_child.shape[1]*w_child.shape[2]*w_child.shape[3]])\n","w_teen = reshape(w_teen, [w_teen.shape[0], w_teen.shape[1]*w_teen.shape[2]*w_teen.shape[3]])\n","w_adult = reshape(w_adult, [w_adult.shape[0], w_adult.shape[1]*w_adult.shape[2]*w_adult.shape[3]])\n","w_old = reshape(w_old, [w_old.shape[0], w_old.shape[1]*w_old.shape[2]*w_old.shape[3]])\n","\n","\n","\n","\n","\n","#z_men = np.array([np.random.RandomState(seed).randn(1, G.z_dim) for seed in men])\n","#w_men = G.mapping(z_men, label, truncation_psi=truncation_psi)\n","#z_women = np.array([np.random.RandomState(seed).randn(1, G.z_dim) for seed in women])\n","\n","color1 = [\"Kinder\" for  i in range(len(child))]\n","#color2 = [\"Teen\" for  i in range(len(teen))]\n","color3 = [\"Erwachsene\" for  i in range(len(adult))]\n","#color4 = [\"Old\" for  i in range(len(old))]\n","\n","#color =  color1 + color2 + color3 + color4\n","color =  color1 + color3 \n","#ws = np.concatenate((w_child, w_teen,w_adult,w_old))\n","ws = np.concatenate((w_child,w_adult))\n","\n","#z_list = [gen_rand_z(G).cpu().numpy().astype(np.float32) for i in range(size)]\n","#np_zs = np.array(z_list)[:,0,:]\n","print(ws.shape)\n","\n","tsne = TSNE(n_components=2, verbose=1, random_state=123, perplexity=49)\n","embed = tsne.fit_transform(ws)\n","\n","#array = [\"Male\", \"Female\"]\n","\n","#color = np.random.randint(0,2,size)\n","#color = random.choices(array, k=size)\n","\n","df = pd.DataFrame()\n","df[\"y\"] = color\n","df[\"comp-1\"] = embed[:,0]\n","df[\"comp-2\"] = embed[:,1]\n","\n","\n","# hue=df.y.tolist()\n","sns.scatterplot(x=\"comp-1\", y=\"comp-2\",hue=df.y.tolist(),\n","                palette=sns.color_palette(\"hls\", 2),\n","                data=df).set(title=\"Alter W Raum\") \n"]},{"cell_type":"markdown","metadata":{"id":"eosHknc9AbLC"},"source":["### Alter Z Spcae\n","Visualization of latenten Codes for different ages in z space"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CIo8rFE3Aa1z"},"outputs":[],"source":["\n","\n","NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","\n","device = 'cuda'\n","truncation_psi = 0.7\n","noise_mode = 'const'\n","\n","men = [0,4,7,8,12,13,14,15,16,18,21,27,28,29,33,34,35,37,40,43,44,45,46,47,48,49,51,52,58,59,65,66,71,72,73,74,75,78,80,82,83,100,103,105,107,108,109,115,123,204,205,206,207,208,324,326,336]\n","women = [6,9,17,22,23,24,25,26,30,31,32,39,41,42,50,53,55,56,57,60,62,64,67,68,70,76,77,79,101,102,104,106,110,111,112,113,114,116,117,118,119,120,121,122,124,127,128,200,201,202,210,211,213,214,222,221,222,223,304,305, 309,346,347,360,361,362,363,364]\n","men2= [540,550,551,555,556,559,568,614,612,623,624,628,635,637,645,648,651,655,662,672,680,681,708,711,712,717,718]\n","women2 = [547,548,549,553,554,557,560,561,563,564,565,569,625,631,632,641,642,650,653,665,682,683,685,688,690,692,694,700,701,702,703,710,713,714,715,720,722,723,728,729]\n","\n","men = men + men2\n","women = women + women2\n","\n","\n","child = [203,209,215,216,217,226,230,322,325,345,400,401,409,415,422,424,425,466,468,479,495,501,499,501,514,516,542,546,573,582,583,592,596,597,630,644,639,640,646,656,677]\n","child2 = [686,689,691,693,696,704,709,719,724,737,]\n","teen = [205,225,245,302,303,313,336,349,344,356]\n","adult = [200,201,202,204,206,207,208,227,228,232,238,240,241,300,402,404,403,405,406,407,410,412,413,414,416,417,418,419,420,421,423,427,428,429,439,431,432,433,434,345]\n","adult2 = [547,548,549,553,554,557,560,561,563,564,540,550,551,555,556,559,568,614,612,623]\n","old = [218,222,224,233,236,249,261,265,309,311,316,334,367,368,411,426,474,540,574,589,638,645,667,655,673,707,717,747,727]\n","\n","child = child + child2\n","adult = old \n","G = init_generator(NETWORK_PKL)\n","label = torch.zeros([1, G.c_dim], device=device)\n","\n","\n","\n","z_child = np.array([np.random.RandomState(seed).randn(1, G.z_dim) for seed in child])\n","z_adult = np.array([np.random.RandomState(seed).randn(1, G.z_dim) for seed in old])\n","\n","\n","z_child = reshape(z_child, [z_child.shape[0], z_child.shape[1]*z_child.shape[2]])\n","#w_teen = reshape(w_teen, [w_teen.shape[0], w_teen.shape[1]*w_teen.shape[2]*w_teen.shape[3]])\n","z_adult = reshape(z_adult, [z_adult.shape[0], z_adult.shape[1]*z_adult.shape[2]])\n","#w_old = reshape(w_old, [w_old.shape[0], w_old.shape[1]*w_old.shape[2]*w_old.shape[3]])\n","\n","\n","\n","\n","\n","#z_men = np.array([np.random.RandomState(seed).randn(1, G.z_dim) for seed in men])\n","#w_men = G.mapping(z_men, label, truncation_psi=truncation_psi)\n","#z_women = np.array([np.random.RandomState(seed).randn(1, G.z_dim) for seed in women])\n","\n","color1 = [\"Kinder\" for  i in range(len(child))]\n","#color2 = [\"Teen\" for  i in range(len(teen))]\n","color3 = [\"Erwachsene\" for  i in range(len(adult))]\n","#color4 = [\"Old\" for  i in range(len(old))]\n","\n","#color =  color1 + color2 + color3 + color4\n","color =  color1 + color3 \n","#ws = np.concatenate((w_child, w_teen,w_adult,w_old))\n","ws = np.concatenate((z_child,z_adult))\n","\n","#z_list = [gen_rand_z(G).cpu().numpy().astype(np.float32) for i in range(size)]\n","#np_zs = np.array(z_list)[:,0,:]\n","print(ws.shape)\n","\n","tsne = TSNE(n_components=2, verbose=1, random_state=123, perplexity=48)\n","embed = tsne.fit_transform(ws)\n","\n","#array = [\"Male\", \"Female\"]\n","\n","#color = np.random.randint(0,2,size)\n","#color = random.choices(array, k=size)\n","\n","df = pd.DataFrame()\n","df[\"y\"] = color\n","df[\"comp-1\"] = embed[:,0]\n","df[\"comp-2\"] = embed[:,1]\n","\n","\n","# hue=df.y.tolist()\n","sns.scatterplot(x=\"comp-1\", y=\"comp-2\",hue=df.y.tolist(),\n","                palette=sns.color_palette(\"hls\", 2),\n","                data=df).set(title=\"Alter im Z Raum\") "]},{"cell_type":"markdown","metadata":{"id":"W4mIKdJVN6rN"},"source":["###Z und W Raum\n","Treid to visualize the overall shape of the images distributed in z space and w space but images where to highdimensional. Results did not contain any meaningful semantics."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M6j5Lq8uOQvc"},"outputs":[],"source":["##Visualisation\n","\n","NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","\n","device = 'cuda'\n","truncation_psi = 0.7\n","noise_mode = 'const'\n","\n","G = init_generator(NETWORK_PKL)\n","label = torch.zeros([1, G.c_dim], device=device)\n","\n","size = 1000\n","img = np.array([G(gen_rand_z(G), label,  truncation_psi=truncation_psi, noise_mode=noise_mode).cpu().numpy().astype(np.float32) for i in range(size)])\n","\n","print( img.shape)\n","img = reshape(img, [img.shape[0], img.shape[1]*img.shape[2]*img.shape[3]*img.shape[4]])\n","print(img.shape)\n","\n","color = [\"1\" for  i in range(size)]\n","\n","tsne = TSNE(n_components=2, verbose=1, random_state=123, perplexity=50)\n","\n","embed_img = tsne.fit_transform(img)\n","\n","df_img = pd.DataFrame()\n","df_img[\"y\"] = color\n","df_img[\"comp-1\"] = embed_img[:,0]\n","df_img[\"comp-2\"] = embed_img[:,1]\n","\n","\n","sns.scatterplot(x=\"comp-1\", y=\"comp-2\",hue=df_img.y.tolist(),\n","                palette=sns.color_palette(\"hls\", 2),\n","                data=df_img).set(title=\"IMG Raum\") \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x5m1BTU3Wq-T"},"outputs":[],"source":["##Visualisation\n","\n","NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","\n","device = 'cuda'\n","truncation_psi = 0.7\n","noise_mode = 'const'\n","\n","G = init_generator(NETWORK_PKL)\n","label = torch.zeros([1, G.c_dim], device=device)\n","\n","size = 1000\n","\n","ws = np.array([G.mapping(gen_rand_z(G), label, truncation_psi=truncation_psi).cpu().numpy().astype(np.float32) for i in range(size)])\n","\n","print(ws.shape)\n","ws = reshape(ws, [ws.shape[0], ws.shape[1]*ws.shape[2]*ws.shape[3]])\n","print(ws.shape)\n","\n","color = [\"1\" for  i in range(size)]\n","\n","tsne = TSNE(n_components=2, verbose=1, random_state=123, perplexity=20)\n","\n","embed_ws = tsne.fit_transform(ws)\n","\n","df_ws = pd.DataFrame()\n","df_ws[\"y\"] = color\n","df_ws[\"comp-1\"] = embed_ws[:,0]\n","df_ws[\"comp-2\"] = embed_ws[:,1]\n","\n","\n","sns.scatterplot(x=\"comp-1\", y=\"comp-2\",hue=df_ws.y.tolist(),\n","                palette=sns.color_palette(\"hls\", 2),\n","                data=df_ws).set(title=\"W Raum\") \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c-2eNcrWWN1V"},"outputs":[],"source":["##Visualisation\n","\n","NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","\n","device = 'cuda'\n","truncation_psi = 0.7\n","noise_mode = 'const'\n","\n","G = init_generator(NETWORK_PKL)\n","label = torch.zeros([1, G.c_dim], device=device)\n","\n","size = 1000\n","\n","zs = np.array([np.random.randn(1, G.z_dim) for i in range(size)])\n","\n","\n","\n","\n","print(zs.shape)\n","zs = reshape(zs, [zs.shape[0], zs.shape[1]*zs.shape[2]])\n","\n","print(zs.shape)\n","\n","\n","color = [\"1\" for  i in range(size)]\n","\n","\n","\n","tsne = TSNE(n_components=2, verbose=1, random_state=123, perplexity=100)\n","embed_zs = tsne.fit_transform(zs)\n","\n","\n","\n","df_zs = pd.DataFrame()\n","df_zs[\"y\"] = color\n","df_zs[\"comp-1\"] = embed_zs[:,0]\n","df_zs[\"comp-2\"] = embed_zs[:,1]\n","\n","\n","\n","sns.scatterplot(x=\"comp-1\", y=\"comp-2\",hue=df_zs.y.tolist(),\n","              palette=sns.color_palette(\"hls\", 2),\n","             data=df_zs).set(title=\"Z Raum\") \n","\n"]},{"cell_type":"markdown","metadata":{"id":"0LyrotP_Pd4v"},"source":["## Arithmetics\n","Use our latent Codes to do vektor arithemtics in latent space. See section 3.2.2 of thesis."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XyCX_NliNzDY"},"outputs":[],"source":["DEVICE = 'cuda'\n","NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","PATH = \"/content/drive/My Drive/Bachelorarbeit/ausarbeitung/gfx/Arithmetik/attributes_w3\"\n","\n","def latent_arithmetic():\n","\n","  # handclasssified seeds \n","  men = [0,4,7,8,12,13,14,15,16,18,21,27,28,29,33,34,35,37,40,43,44,45,46,47,48,49,51,52,58,59,65,66,71,72,73,74,75,78,80,82,83,100,103,105,107,108,109,115,123,204,205,206,207,208,324,326,336]\n","  women = [6,9,17,22,23,24,25,26,30,31,32,39,41,42,50,53,55,56,57,60,62,64,67,68,70,76,77,79,101,102,104,106,110,111,112,113,114,116,117,118,119,120,121,122,124,127,128,200,201,202,210,211,213,214,222,221,222,223,304,305, 309,346,347,360,361,362,363,364]\n","  men2= [540,550,551,555,556,559,568,614,612,623,624,628,635,637,645,648,651,655,662,672,680,681,708,711,712,717,718]\n","  women2 = [547,548,549,553,554,557,560,561,563,564,565,569,625,631,632,641,642,650,653,665,682,683,685,688,690,692,694,700,701,702,703,710,713,714,715,720,722,723,728,729]\n","\n","  men = men + men2\n","  women = women + women2\n","  person = men + women\n","\n","  child = [203,209,215,216,217,226,230,322,325,345,400,401,409,415,422,424,425,466,468,479,495,501,499,501,514,516,542,546,573,582,583,592,596,597,630,644,639,640,646,656,677]\n","  child2 = [686,689,691,693,696,704,709,719,724,737]\n","\n","  glasses_men =[206, 311, 334, 336, 339, 348, 352, 355, 356, 373, 374, 376, 775, 758,776,780,788,790,796,821,831,840,843,846,878,891,915, 916,917,944, 962, 965, 970, 975, 988,990,993,1006,517]\n","  glasses_women =[255, 295, 291, 286, 309, 347, 384,683,770,798,851,874,900,935,977,1001,513,195]\n","  smiling_men = [256,259, 273, 275, 300, 316, 320, 324]\n","  smiling_women = [210, 214, 260, 272, 274, 276, 302, 309, 313, 338, 347,346, 350]\n","  neutral_men = [218,257,258, 261, 264, 265, 307, 318, 319]\n","  neutral_women = [211, 310, 312, 334, 351, 359]\n","  smiling_child = [209, 216, 366]\n","  neutral_child = [322,325,328, 341, 345, 356, 357]\n","  beard = [218, 308, 318, 373,970,1013,16,123,181]\n","  lipstick = [222, 225, 303, 305, 310, 763,888,907,1017,1029,547,198,192]\n","  darker_skin_women = [272, 290, 384, 504,121,128]\n","  darker_skin_men = [229, 271, 348, 349, 375, 399, 386,804,807,824,860,80,82]\n","\n","  darker_skin = darker_skin_women + darker_skin_men\n","  glasses = glasses_men + glasses_women\n","  smiling = smiling_men + smiling_women\n","  child = smiling_child + neutral_child + child + child2\n","\n","\n","  #init hyperparameters and generator\n","  truncation_psi = 0.7\n","  noise_mode = 'random'\n","\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=DEVICE)\n","\n","  #compute the average latent code for an attribute and plot the image generated by the average code.\n","  w_smiling_men =  [G.mapping(torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE), label, truncation_psi=truncation_psi, truncation_cutoff=8 ) for seed in smiling_men]\n","  w_smiling_men = torch.cat([l for l in w_smiling_men]).to(DEVICE)\n","  average_smiling_men = torch.mean(w_smiling_men, 0, True)\n","  img_average_smiling_men = G.synthesis(average_smiling_men, noise_mode=noise_mode, force_fp32=False)\n","  plot_generator_img(img_average_smiling_men, \"average_smiling_men\")\n","\n","  w_men =  [G.mapping(torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE),label, truncation_psi=truncation_psi, truncation_cutoff=8 ) for seed in men]\n","  w_men = torch.cat([l for l in w_men]).to(DEVICE)\n","  average_men = torch.mean(w_men, 0, True)\n","  img_average_men = G.synthesis(average_men, noise_mode=noise_mode, force_fp32=False)\n","  plot_generator_img(img_average_men, \"average_men\")\n","\n","  w_women =  [G.mapping(torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE),label, truncation_psi=truncation_psi, truncation_cutoff=8 ) for seed in women]\n","  w_women = torch.cat([l for l in w_women]).to(DEVICE)\n","  average_women = torch.mean(w_women, 0, True)\n","  img_average_women = G.synthesis(average_women, noise_mode=noise_mode, force_fp32=False)\n","  plot_generator_img(img_average_women, \"average_women\")\n","\n","  w_child =  [G.mapping(torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE),label, truncation_psi=truncation_psi, truncation_cutoff=8 ) for seed in child]\n","  w_child = torch.cat([l for l in w_child]).to(DEVICE)\n","  average_child = torch.mean(w_child, 0, True)\n","  img_average_child = G.synthesis(average_child, noise_mode=noise_mode, force_fp32=False)\n","  plot_generator_img(img_average_child, \"average_child\")\n","\n","  w_lipstick =  [G.mapping(torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE),label, truncation_psi=truncation_psi, truncation_cutoff=8 ) for seed in lipstick]\n","  w_lipstick = torch.cat([l for l in w_lipstick]).to(DEVICE)\n","  average_lipstick = torch.mean(w_lipstick, 0, True)\n","  img_average_lipstick = G.synthesis(average_lipstick, noise_mode=noise_mode, force_fp32=False)\n","  plot_generator_img(img_average_lipstick, \"average_lipstick\")\n","\n","  w_men_glasses =  [G.mapping(torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE),label, truncation_psi=truncation_psi, truncation_cutoff=8 ) for seed in glasses_men]\n","  w_men_glasses = torch.cat([l for l in w_men_glasses]).to(DEVICE)\n","  average_men_glasses = torch.mean(w_men_glasses, 0, True)\n","  img_average_men_glasses = G.synthesis(average_men_glasses, noise_mode=noise_mode, force_fp32=False)\n","  plot_generator_img(img_average_men_glasses, \"men with glasses\")\n","\n","  w_person =  [G.mapping(torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE),label, truncation_psi=truncation_psi, truncation_cutoff=8 ) for seed in person]\n","  w_person = torch.cat([l for l in w_person]).to(DEVICE)\n","  average_person = torch.mean(w_person, 0, True)\n","  img_average_person = G.synthesis(average_person, noise_mode=noise_mode, force_fp32=False)\n","  plot_generator_img(img_average_person, \"average person\")\n","\n","  w_glasses =  [G.mapping(torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE),label, truncation_psi=truncation_psi, truncation_cutoff=8 ) for seed in glasses]\n","  w_glasses = torch.cat([l for l in w_glasses]).to(DEVICE)\n","  average_glasses = torch.mean(w_glasses, 0, True)\n","  img_average_glasses = G.synthesis(average_glasses, noise_mode=noise_mode, force_fp32=False)\n","  plot_generator_img(img_average_glasses, \"person with glasses\")\n","\n","  w_beard =  [G.mapping(torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE),label, truncation_psi=truncation_psi, truncation_cutoff=8 ) for seed in beard]\n","  w_beard = torch.cat([l for l in w_beard]).to(DEVICE)\n","  average_beard = torch.mean(w_beard, 0, True)\n","  img_average_beard = G.synthesis(average_beard, noise_mode=noise_mode, force_fp32=False)\n","  plot_generator_img(img_average_beard, \"men with beard\")\n","\n","  child_glasses = average_men_glasses - average_men + average_child\n","  img_child_glasses = G.synthesis(child_glasses, noise_mode=noise_mode, force_fp32=False)\n","  plot_generator_img(img_child_glasses, \"child with glasses\")\n","\n","  #compute subspace for image manipulation\n","  lipstick = average_lipstick - average_women\n","  glasses = average_glasses - average_person\n","  beard = average_beard - average_men\n","\n","  #Uncomment if u want to use your own latent code from a torch Tensor to be manipulated\n","  #w_load = load_tensor(\"/content/drive/MyDrive/Bachelorarbeit/ausarbeitung/gfx/projection/w_tensors/macron_with_noise.pt\")\n","\n","  #w_load_beard = w_load +  beard\n","  #w_load_glasses = w_load +  glasses \n","  #w_load_lipstick = w_load +  lipstick\n","  \n","  #img_load = G.synthesis(w_load, noise_mode=noise_mode, force_fp32=False)\n","  #img_load_beard = G.synthesis(w_load_beard, noise_mode=noise_mode, force_fp32=False)\n","  #img_load_glasses = G.synthesis(w_load_glasses, noise_mode=noise_mode, force_fp32=False)\n","  #img_load_lipstick = G.synthesis(w_load_lipstick, noise_mode=noise_mode, force_fp32=False)\n","\n","  #plot_generator_img(img_load, \"loaded image\")\n","  #plot_generator_img(img_load_beard, \"loaded image with beard\")\n","  #plot_generator_img(img_load_glasses, \"loaded image with glasses\")\n","  #plot_generator_img(img_load_lipstick, \"loaded image with makeup\")\n","\n","  #Use computed subspace to perform manipulation on random images\n","  for i in range(15):\n","    test_men = G.mapping(torch.from_numpy(np.random.RandomState(1300+i).randn(1, G.z_dim)).to(DEVICE),label, truncation_psi=truncation_psi, truncation_cutoff=8 )\n","    img_test_men =  G.synthesis(test_men, noise_mode=noise_mode, force_fp32=False)\n","    plot_generator_img(img_test_men, \"test men\")\n","\n","    men_lipstick = test_men + lipstick\n","    img_men_lipstick = G.synthesis(men_lipstick, noise_mode=noise_mode, force_fp32=False)\n","    plot_generator_img(img_men_lipstick, \"men with lipstick\")\n","\n","    men_beard = test_men + beard\n","    img_men_beard = G.synthesis(men_beard, noise_mode=noise_mode, force_fp32=False)\n","    plot_generator_img(img_men_beard, \"men with beard\")\n","\n","    men_glasses = test_men + glasses\n","    img_men_glasses = G.synthesis(men_glasses, noise_mode=noise_mode, force_fp32=False)\n","    plot_generator_img(img_men_glasses, \"test men with glasses\")\n","    save_generator_img(img_men_glasses, f'{PATH}/men_glasses-{i}.png')\n","    save_generator_img(img_men_lipstick, f'{PATH}/men_lipstick-{i}.png')\n","    save_generator_img(img_men_beard, f'{PATH}/men_beard-{i}.png')\n","    save_generator_img(img_test_men, f'{PATH}/original_men-{i}.png')\n","\n","\n","  for i in range(10):\n","    test_women = G.mapping(torch.from_numpy(np.random.RandomState(1200+i).randn(1, G.z_dim)).to(DEVICE),label, truncation_psi=truncation_psi, truncation_cutoff=8 )\n","    img_original =  G.synthesis(test_women, noise_mode=noise_mode, force_fp32=False)\n","    plot_generator_img(img_original, \"test women\")\n","\n","    test_women_lipstick = test_women + lipstick\n","    img_test_lipstick =  G.synthesis(test_women_lipstick, noise_mode=noise_mode, force_fp32=False)\n","    plot_generator_img(img_test_lipstick, \"test lipstick women\")\n","\n","    test_women_glasses = test_women + glasses\n","    img_test_women_glasses =  G.synthesis(test_women_glasses, noise_mode=noise_mode, force_fp32=False)\n","\n","    save_generator_img(img_original, f'{PATH}/original_women-{i}.png')\n","    save_generator_img(img_test_lipstick, f'{PATH}/women_lipstick-{i}.png')\n","    save_generator_img(img_test_women_glasses, f'{PATH}/women_glasses-{i}.png')\n","\n","\n","\n","latent_arithmetic()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lJ8fh7GAODY5"},"outputs":[],"source":["latent_arithmetic()"]},{"cell_type":"markdown","metadata":{"id":"BLKQI0CWEx9r"},"source":["## Arithmetics in Z\n","Same as Section Arithmetics but in z-space to test our hypothesis of thesis section 3.2.2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uX_ScZLAE3UA"},"outputs":[],"source":["DEVICE = 'cuda'\n","NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","PATH = \"/content/drive/My Drive/Bachelorarbeit/ausarbeitung/gfx/Arithmetik/attributes_z3\"\n","\n","def latent_arithmetic_z():\n","\n","\n","  men = [0,4,7,8,12,13,14,15,16,18,21,27,28,29,33,34,35,37,40,43,44,45,46,47,48,49,51,52,58,59,65,66,71,72,73,74,75,78,80,82,83,100,103,105,107,108,109,115,123,204,205,206,207,208,324,326,336]\n","  women = [6,9,17,22,23,24,25,26,30,31,32,39,41,42,50,53,55,56,57,60,62,64,67,68,70,76,77,79,101,102,104,106,110,111,112,113,114,116,117,118,119,120,121,122,124,127,128,200,201,202,210,211,213,214,222,221,222,223,304,305, 309,346,347,360,361,362,363,364]\n","  men2= [540,550,551,555,556,559,568,614,612,623,624,628,635,637,645,648,651,655,662,672,680,681,708,711,712,717,718]\n","  women2 = [547,548,549,553,554,557,560,561,563,564,565,569,625,631,632,641,642,650,653,665,682,683,685,688,690,692,694,700,701,702,703,710,713,714,715,720,722,723,728,729]\n","\n","  men = men + men2\n","  women = women +  women2\n","  person = men + women\n","\n","  child = [203,209,215,216,217,226,230,322,325,345,400,401,409,415,422,424,425,466,468,479,495,501,499,501,514,516,542,546,573,582,583,592,596,597,630,644,639,640,646,656,677]\n","  child2 = [686,689,691,693,696,704,709,719,724,737]\n","\n","  glasses_men =[206, 311, 334, 336, 339, 348, 352, 355, 356, 373, 374, 376, 775, 758,776,780,788,790,796,821,831,840,843,846,878,891,915, 916,917,944, 962, 965, 970, 975, 988,990,993,1006,517]\n","  glasses_women =[255, 295, 291, 286, 309, 347, 384,683,770,798,851,874,900,935,977,1001,513,195]\n","  smiling_men = [256,259, 273, 275, 300, 316, 320, 324]\n","  smiling_women = [210, 214, 260, 272, 274, 276, 302, 309, 313, 338, 347,346, 350]\n","  neutral_men = [218,257,258, 261, 264, 265, 307, 318, 319]\n","  neutral_women = [211, 310, 312, 334, 351, 359]\n","  smiling_child = [209, 216, 366]\n","  neutral_child = [322,325,328, 341, 345, 356, 357]\n","  beard = [218, 308, 318, 373,970,1013,16,123,181]\n","  lipstick = [222, 225, 303, 305, 310, 763,888,907,1017,1029,547,198,192]\n","  darker_skin_women = [272, 290, 384, 504,121,128]\n","  darker_skin_men = [229, 271, 348, 349, 375, 399, 386,804,807,824,860,80,82]\n","\n","  darker_skin = darker_skin_women + darker_skin_men\n","  glasses = glasses_men + glasses_women\n","  smiling = smiling_men + smiling_women\n","  child = smiling_child + neutral_child + child + child2\n","\n","\n","  truncation_psi = 0.7\n","  noise_mode = 'const'\n","\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=DEVICE)\n","\n","\n","  z_men =  [torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)) for seed in men]\n","  z_men = torch.cat([l for l in z_men]).to(DEVICE)\n","  average_men = torch.mean(z_men, 0, True)\n","  img_average_men = G(average_men, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","  plot_generator_img(img_average_men, \"average_men\")\n","\n","  z_women =  [torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)) for seed in women]\n","  z_women = torch.cat([l for l in z_women]).to(DEVICE)\n","  average_women = torch.mean(z_women, 0, True)\n","  img_average_women = G(average_women, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","  plot_generator_img(img_average_women, \"average_women\")\n","\n","  z_lipstick =  [torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE) for seed in lipstick]\n","  z_lipstick = torch.cat([l for l in z_lipstick]).to(DEVICE)\n","  average_lipstick = torch.mean(z_lipstick, 0, True)\n","  img_average_lipstick = G(average_lipstick, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","  plot_generator_img(img_average_lipstick, \"average_lipstick\")\n","\n","  z_person =  [torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE) for seed in person]\n","  z_person = torch.cat([l for l in z_person]).to(DEVICE)\n","  average_person = torch.mean(z_person, 0, True)\n","  img_average_person = G(average_person, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","  plot_generator_img(img_average_person, \"average person\")\n","\n","  z_glasses =  [torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE) for seed in glasses]\n","  z_glasses = torch.cat([l for l in z_glasses]).to(DEVICE)\n","  average_glasses = torch.mean(z_glasses, 0, True)\n","  img_average_glasses = G(average_glasses, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","  plot_generator_img(img_average_glasses, \"person with glasses\")\n","\n","  z_beard =  [torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE) for seed in beard]\n","  z_beard = torch.cat([l for l in z_beard]).to(DEVICE)\n","  average_beard = torch.mean(z_beard, 0, True)\n","  img_average_beard = G(average_beard, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","  plot_generator_img(img_average_beard, \"men with beard\")\n","\n","\n","  lipstick = average_lipstick - average_women\n","  lipstick = lipstick.to(DEVICE)\n","  glasses = average_glasses - average_person\n","  glasses = glasses.to(DEVICE)\n","  beard = average_beard - average_men\n","  beard = beard.to(DEVICE)\n","\n","\n","  seeds = []\n","  #uncomment if you want every image plotted\n","  for i in range(15):\n","    test_men = torch.from_numpy(np.random.RandomState(1300+i).randn(1, G.z_dim)).to(DEVICE)\n","    img_test_men =  G(test_men, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","    #plot_generator_img(img_test_men, \"test men\")\n","\n","    men_lipstick = test_men + lipstick\n","    img_men_lipstick =G(men_lipstick, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","    #plot_generator_img(img_men_lipstick, \"men with lipstick\")\n","\n","    men_beard = test_men + beard\n","    img_men_beard = G(men_beard, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","    #plot_generator_img(img_men_beard, \"men with beard\")\n","\n","    men_glasses = test_men + glasses\n","    img_men_glasses = G(men_glasses, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","    #plot_generator_img(img_men_glasses, \"test men with glasses\")\n","    save_generator_img(img_men_glasses, f'{PATH}/men_glasses-{i}.png')\n","    save_generator_img(img_men_lipstick, f'{PATH}/men_lipstick-{i}.png')\n","    save_generator_img(img_men_beard, f'{PATH}/men_beard-{i}.png')\n","    save_generator_img(img_test_men, f'{PATH}/original_men-{i}.png')\n","\n","  #uncomment for more images (only female)\n","  #for i in range(10):\n","   # test_women =torch.from_numpy(np.random.RandomState(women[10+i]).randn(1, G.z_dim)).to(DEVICE)\n","   # img_original =  G(test_women, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","    #plot_generator_img(img_original, \"test women\")\n","\n","    #test_women_lipstick = test_women + lipstick\n","    #img_test_lipstick =  G(test_women_lipstick, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","    #plot_generator_img(img_test_lipstick, \"test lipstick women\")\n","\n","    #test_women_glasses = test_women + glasses\n","    #img_test_women_glasses =  G(test_women_glasses, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","    #plot_generator_img(img_test_lipstick, \"test women glasses\")\n","\n","    #save_generator_img(img_original, f'{PATH}/original_women-{i}.png')\n","    #save_generator_img(img_test_lipstick, f'{PATH}/women_lipstick-{i}.png')\n","    #save_generator_img(img_test_women_glasses, f'{PATH}/women_glasses-{i}.png')\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Z1kDPHKFnmT"},"outputs":[],"source":["latent_arithmetic_z()"]},{"cell_type":"markdown","metadata":{"id":"f4Lf1vOUsvZn"},"source":["#Projection"]},{"cell_type":"markdown","metadata":{"id":"fG4lDbqGtyJc"},"source":["##Preprocessing\n","Reference: https://github.com/NVlabs/ffhq-dataset/blob/master/download_ffhq.py"]},{"cell_type":"markdown","metadata":{"id":"OcYShtuMuAQ1"},"source":["###Downloads\n","Only need to be executed once. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LPAWXVlHtxyk"},"outputs":[],"source":["!pip install Google-Colab-Transfer\n","import colab_transfer\n","colab_transfer.mount_google_drive()\n","\n","model_file = 'shape_predictor_68_face_landmarks.dat.bz2'\n","\n","!wget http://dlib.net/files/$model_file\n","colab_transfer.copy_file(file_name=model_file,\n","                         destination='/content/drive/My Drive/Bachelorarbeit/bachelorarbeit/stylegan3/project_images')"]},{"cell_type":"markdown","metadata":{"id":"3qcY4yrguyiF"},"source":["###Preprocessing functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ATHdZ58ysykM"},"outputs":[],"source":["def detect_face_landmarks(face_file_path=None,\n","                          predictor_path=None,\n","                          img=None):\n","  # References:\n","  # -   http://dlib.net/face_landmark_detection.py.html\n","  # -   http://dlib.net/face_alignment.py.html\n","\n","  if predictor_path is None:\n","    predictor_path = '/content/drive/My Drive/Bachelorarbeit/bachelorarbeit/stylegan3/project_images/shape_predictor_68_face_landmarks.dat'\n","\n","  # Load all the models we need: a detector to find the faces, a shape predictor\n","  # to find face landmarks so we can precisely localize the face\n","  detector = dlib.get_frontal_face_detector()\n","  shape_predictor = dlib.shape_predictor(predictor_path)\n","\n","  if img is None:\n","    # Load the image using Dlib\n","    print(\"Processing file: {}\".format(face_file_path))\n","    img = dlib.load_rgb_image(face_file_path)\n","\n","  shapes = list()\n","\n","  # Ask the detector to find the bounding boxes of each face. The 1 in the\n","  # second argument indicates that we should upsample the image 1 time. This\n","  # will make everything bigger and allow us to detect more faces.\n","  dets = detector(img, 1)\n","    \n","  num_faces = len(dets)\n","  print(\"Number of faces detected: {}\".format(num_faces))\n","\n","  # Find the face landmarks we need to do the alignment.\n","  faces = dlib.full_object_detections()\n","  for d in dets:\n","      print(\"Left: {} Top: {} Right: {} Bottom: {}\".format(\n","          d.left(), d.top(), d.right(), d.bottom()\n","      ))\n","\n","      shape = shape_predictor(img, d)\n","      faces.append(shape)\n","\n","  return faces"]},{"cell_type":"markdown","metadata":{"id":"3DOlD25Yu-WE"},"source":["Displaying Landmarks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z0txjzBGvBMF"},"outputs":[],"source":["import collections\n","\n","plot_style = dict(marker='o',\n","                  markersize=4,\n","                  linestyle='-',\n","                  lw=2)\n","\n","pred_type = collections.namedtuple('prediction_type', ['slice', 'color'])\n","pred_types = {'face': pred_type(slice(0, 17), (0.682, 0.780, 0.909, 0.5)),\n","              'eyebrow1': pred_type(slice(17, 22), (1.0, 0.498, 0.055, 0.4)),\n","              'eyebrow2': pred_type(slice(22, 27), (1.0, 0.498, 0.055, 0.4)),\n","              'nose': pred_type(slice(27, 31), (0.345, 0.239, 0.443, 0.4)),\n","              'nostril': pred_type(slice(31, 36), (0.345, 0.239, 0.443, 0.4)),\n","              'eye1': pred_type(slice(36, 42), (0.596, 0.875, 0.541, 0.3)),\n","              'eye2': pred_type(slice(42, 48), (0.596, 0.875, 0.541, 0.3)),\n","              'lips': pred_type(slice(48, 60), (0.596, 0.875, 0.541, 0.3)),\n","              'teeth': pred_type(slice(60, 68), (0.596, 0.875, 0.541, 0.4))\n","              }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gy1VaXW1vPoM"},"outputs":[],"source":["def display_landmarks_raw(input_img, preds=None, fig_size=None):\n","  # This is a raw copy from:\n","  # https://github.com/1adrianb/face-alignment/blob/master/examples/detect_landmarks_in_image.py\n","\n","  if fig_size is None:\n","    fig_size = plt.figaspect(.5)\n","\n","  fig = plt.figure(figsize=fig_size)\n","  ax = fig.add_subplot(1, 1, 1) # only display one image\n","  ax.imshow(input_img)\n","\n","  if preds is not None:\n","    for pred_type in pred_types.values():\n","        ax.plot(preds[pred_type.slice, 0],\n","                preds[pred_type.slice, 1],\n","                color=pred_type.color, **plot_style)\n","\n","  ax.axis('off')\n","\n","  return"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"678YmG2KvYit"},"outputs":[],"source":["def display_landmarks(image_name, \n","                      dlib_output_faces=None, \n","                      face_no=0,\n","                      fig_size=None):\n","  \n","  if fig_size is None:\n","    fig_size = [15, 15]\n","\n","  input_img = ios.imread(image_name)\n","\n","  if dlib_output_faces is None:\n","    dlib_output_faces = detect_face_landmarks(face_file_path=image_name,\n","                                              img=input_img)\n","\n","  try:\n","    current_face = dlib_output_faces[face_no]\n","\n","  except IndexError:\n","    current_face = None\n","\n","    print('No face found for index n°{} (max={}).'.format(\n","        face_no, \n","        len(dlib_output_faces)-1,\n","        ))\n","\n","  if current_face is None:\n","    preds = None\n","  else:\n","    face_parts = current_face.parts()\n","    \n","    preds = np.array([\n","                      [v.x, v.y] \n","                      for v in face_parts\n","                      ])    \n","    \n","  display_landmarks_raw(input_img=input_img, \n","                        preds=preds,\n","                        fig_size=fig_size)  \n","\n","  return"]},{"cell_type":"markdown","metadata":{"id":"qsv0YYv0vgt9"},"source":["Align with Landmmarks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0gBlI1ravkPC"},"outputs":[],"source":["def recreate_aligned_images(json_data,\n","                            save=False,\n","                            output_path=None,\n","                            output_size=256, \n","                            transform_size=4096, \n","                            enable_padding=True):\n","    #print('Recreating aligned images...')\n","    #if dst_dir:\n","    #    os.makedirs(dst_dir, exist_ok=True)\n","\n","    for item_idx, item in enumerate(json_data.values()):\n","        print('\\r%d / %d ... ' % (item_idx, len(json_data)), end='', flush=True)\n","\n","        # Parse landmarks.\n","        # pylint: disable=unused-variable\n","        lm = np.array(item['in_the_wild']['face_landmarks'])\n","        lm_chin          = lm[0  : 17]  # left-right\n","        lm_eyebrow_left  = lm[17 : 22]  # left-right\n","        lm_eyebrow_right = lm[22 : 27]  # left-right\n","        lm_nose          = lm[27 : 31]  # top-down\n","        lm_nostrils      = lm[31 : 36]  # top-down\n","        lm_eye_left      = lm[36 : 42]  # left-clockwise\n","        lm_eye_right     = lm[42 : 48]  # left-clockwise\n","        lm_mouth_outer   = lm[48 : 60]  # left-clockwise\n","        lm_mouth_inner   = lm[60 : 68]  # left-clockwise\n","\n","        # Calculate auxiliary vectors.\n","        eye_left     = np.mean(lm_eye_left, axis=0)\n","        eye_right    = np.mean(lm_eye_right, axis=0)\n","        eye_avg      = (eye_left + eye_right) * 0.5\n","        eye_to_eye   = eye_right - eye_left\n","        mouth_left   = lm_mouth_outer[0]\n","        mouth_right  = lm_mouth_outer[6]\n","        mouth_avg    = (mouth_left + mouth_right) * 0.5\n","        eye_to_mouth = mouth_avg - eye_avg\n","\n","        # Choose oriented crop rectangle.\n","        print(eye_to_mouth.shape)\n","        x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n","        x /= np.hypot(*x)\n","        x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n","        y = np.flipud(x) * [-1, 1]\n","        c = eye_avg + eye_to_mouth * 0.1\n","        quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n","        qsize = np.hypot(*x) * 2\n","\n","        # Load in-the-wild image.\n","        src_file = item['in_the_wild']['file_path']\n","        img = PIL.Image.open(src_file)\n","\n","        # Shrink.\n","        shrink = int(np.floor(qsize / output_size * 0.5))\n","        if shrink > 1:\n","            rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))\n","            img = img.resize(rsize, PIL.Image.ANTIALIAS)\n","            quad /= shrink\n","            qsize /= shrink\n","\n","        # Crop.\n","        border = max(int(np.rint(qsize * 0.1)), 3)\n","        crop = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n","        crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]), min(crop[3] + border, img.size[1]))\n","        if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:\n","            img = img.crop(crop)\n","            quad -= crop[0:2]\n","\n","        # Pad.\n","        pad = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n","        pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0), max(pad[3] - img.size[1] + border, 0))\n","        if enable_padding and max(pad) > border - 4:\n","            pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n","            img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n","            h, w, _ = img.shape\n","            y, x, _ = np.ogrid[:h, :w, :1]\n","            mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w-1-x) / pad[2]), 1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h-1-y) / pad[3]))\n","            blur = qsize * 0.02\n","            img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n","            img += (np.median(img, axis=(0,1)) - img) * np.clip(mask, 0.0, 1.0)\n","            img = PIL.Image.fromarray(np.uint8(np.clip(np.rint(img), 0, 255)), 'RGB')\n","            quad += pad[:2]\n","\n","        # Transform.\n","        img = img.transform((transform_size, transform_size), PIL.Image.QUAD, (quad + 0.5).flatten(), PIL.Image.BILINEAR)\n","        if output_size < transform_size:\n","            img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)\n","\n","        # Save aligned image.\n","        if(save == True and output_path != None):\n","          img.save(output_path)\n","          return img\n","        else:\n","          print(\"returned image\")\n","          return img\n","\n","    # All done.\n","    print('\\r%d / %d ... done' % (len(json_data), len(json_data)))\n","\n","    return"]},{"cell_type":"markdown","metadata":{"id":"xjUXLk1UvxPz"},"source":["Full Workflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MyZzyMEov1Td"},"outputs":[],"source":["face_file_path = \"/content/drive/My Drive/Bachelorarbeit/bachelorarbeit/stylegan3/project_images/target_images/hemsworth.jpg\"\n","faces = detect_face_landmarks(face_file_path=face_file_path)\n","\n","#Display landmarks on face\n","face_no=0\n","fig_size=[15,15]\n","\n","display_landmarks(image_name=face_file_path,\n","                  dlib_output_faces=faces,\n","                  face_no=face_no,\n","                  fig_size=fig_size)\n","\n","# The first face which is detected:\n","# NB: we assume that there is exactly one face per picture!\n","f = faces[0]\n","\n","parts = f.parts()\n","\n","num_face_landmarks=68\n","\n","v = np.zeros(shape=(num_face_landmarks, 2))\n","for k, e in enumerate(parts):\n","  v[k, :] = [e.x, e.y]\n","\n","json_data = dict()\n","\n","item_idx = 0\n","\n","json_data[item_idx] = dict()\n","json_data[item_idx]['in_the_wild'] = dict()\n","json_data[item_idx]['in_the_wild']['file_path'] = face_file_path\n","json_data[item_idx]['in_the_wild']['face_landmarks'] = v\n","\n","recreate_aligned_images(json_data, \"/content/drive/My Drive/Bachelorarbeit/bachelorarbeit/stylegan3/project_images/aligned_target_images/img12.png\")"]},{"cell_type":"markdown","metadata":{"id":"51okfi2Jwghk"},"source":["##Preprocessing Pipeline\n","Combines all preprocessing steps\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kito37XhwmNE"},"outputs":[],"source":["\n","def preprocess_pipeline(img_path, save=False, out_path=None):\n","  face_file_path = img_path\n","  faces = detect_face_landmarks(face_file_path=face_file_path)\n","  face_no=0\n","  fig_size=[15,15]\n","\n","  display_landmarks(image_name=face_file_path,\n","                    dlib_output_faces=faces,\n","                    face_no=face_no,\n","                    fig_size=fig_size)\n","  \n","  # The first face which is detected:\n","  # NB: we assume that there is exactly one face per picture!\n","  f = faces[0]\n","\n","  parts = f.parts()\n","\n","  num_face_landmarks=68\n","\n","  v = np.zeros(shape=(num_face_landmarks, 2))\n","  for k, e in enumerate(parts):\n","    v[k, :] = [e.x, e.y]\n","\n","  json_data = dict()\n","\n","  item_idx = 0\n","\n","  json_data[item_idx] = dict()\n","  json_data[item_idx]['in_the_wild'] = dict()\n","  json_data[item_idx]['in_the_wild']['file_path'] = face_file_path\n","  json_data[item_idx]['in_the_wild']['face_landmarks'] = v\n","\n"," \n","  img = recreate_aligned_images(json_data, save, out_path)\n","  \n","  return img"]},{"cell_type":"markdown","metadata":{"id":"xtCvBBNsZwWe"},"source":["## Projection into w-space\n","First attempt at projection into w-sapce without preprocessing. Can only be used for 256x256 Images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x02aslpCZy1l"},"outputs":[],"source":["NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","DEVICE = 'cuda'\n","\n","def projection_w(img_path: Union[str, os.PathLike, BinaryIO, IO[bytes]],\n","                 num_epochs: int,\n","                 learning_rate,\n","                 save_path_projection: Union[str, os.PathLike, BinaryIO, IO[bytes]],\n","                 name,\n","                 save_path_aligned: Union[str, os.PathLike, BinaryIO, IO[bytes]] = None,\n","                 continue_training: bool = False,\n","                 pre_trained_image_path: Union[str, os.PathLike, BinaryIO, IO[bytes]] = None):\n","  \n","\n","\n","  #Get Image in the right tensor shape and plot target image\n","  target_img = prepare_image(img_path, 256)\n","  plot_generator_img(target_img, \"Target Image\")\n","\n","  #Init Generator\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=DEVICE)\n","  \n","  truncation_psi = 0.7\n","  noise_mode = 'const'\n","\n","  #Init loss functions, only vgg ued, combinations of vgg and mse where used during testing but didn't yield better results\n","  loss_fn_mse = torch.nn.MSELoss()\n","  loss_fn_vgg = VGGPerceptualLoss().to(DEVICE)\n","  \n","  if(continue_training == False ):\n","      #Initiliza array of random image and use the best to project to\n","      w_list = [G.mapping(gen_rand_z(G), label) for i in range(1)]\n","      w_img_list = [G.synthesis(w, noise_mode=noise_mode, force_fp32=False).to(DEVICE) for w in w_list]\n","\n","      #Calculate loss for each image\n","      w_img_error_list = [loss_fn_vgg(w_img, target_img) for w_img in w_img_list]\n","\n","\n","      #sort list in ascending order, after that the first element is the best\n","      w_list_sorted = sorted(zip(w_list, w_img_error_list), key=lambda x: x[1])\n","\n","      #copy best image to w\n","      w = w_list_sorted[0][0].clone().detach().requires_grad_(True)\n","\n","  else:\n","      #load pre trained image\n","      w = load_tensor(pre_trained_image_path).clone().detach().requires_grad_(True)\n","\n","  #initialize optimizer/scheduler\n","  learning_rate = learning_rate\n","  optimizer = torch.optim.Adam([w], lr=learning_rate)\n","\n","  #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n","  #scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.995)\n","  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=50, epochs=int(num_epochs/50))\n","\n","\n","  new_img = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","  plot_generator_img(new_img, \"Starting  Point \" )\n","\n","\n","  avg_loss = 0\n","  plot_loss = []\n","\n","  #Projection Step\n","  for epoch in range(num_epochs):\n","    optimizer.zero_grad()\n","    #Generate image from w\n","    pred = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","    #Calculate loss\n","    #loss_mse = loss_fn_mse(pred, target_img)\n","    loss_vgg = loss_fn_vgg(pred, target_img)\n","    loss =  loss_vgg\n","    avg_loss += loss \n","\n","    #Optimize w\n","    loss.backward()\n","    optimizer.step()\n","    scheduler.step()\n","\n","    #Print loss and current state of the projection\n","    if epoch % 100 == 0:\n","        print('Epoch: %d, avgloss: %f' % (epoch, avg_loss))\n","        print(loss_vgg)\n","        new_img = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","        plot_generator_img(new_img, \"Projection after \" +  str(epoch) +  \" epochs: \" )\n","        plot_loss.append(avg_loss)\n","        avg_loss = 0\n","\n","  #Plot loss during projection (not used in thesis, as the graph for different setups doesn't yield comparable information)\n","  plot_loss.append(avg_loss)\n","  plot_loss = [x.cpu().detach().numpy() for x in plot_loss]\n","  plot_loss = plot_loss[1:]\n","  plot_x = [x for  x in range(0, num_epochs+1, 100)]\n","  plot_x = plot_x[1:]\n","  plt.plot(plot_x, plot_loss)\n","  plt.xlabel(\"num_epochs\")\n","  plt.ylabel(\"avg_loss in last 100 epochs\")\n","  plt.title(\"Loss during training\")\n","  plt.show()\n","\n","  #Plot and save final image and optimised latent code\n","  final_img = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","  plot_generator_img(final_img, 'Projiziertes Bild')\n","  plot_generator_img(target_img, 'Original Bild')\n","\n","  save_tensor(w, save_path_projection + f'/w_tensors/{name}.pt')\n","  save_generator_img(final_img,  save_path_projection + f'/images/{name}.png')\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bUWtrMnkgWaa"},"outputs":[],"source":["learning_rates = [0.002,0.005,0.01,0.02, 0.05, 0.1, 0.2]\n","\n","\n","projection_w(\"/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/projection/target_images/cat.jpg\",\n","            2000,\n","            0.002,\n","            \"/content/drive/MyDrive/Bachelorarbeit/ausarbeitung/gfx/projection\",\n","            'hemsworth_ohne_preprocessing')"]},{"cell_type":"markdown","metadata":{"id":"p9zjK-Rx82cO"},"source":["## Projection into z-space\n","Projection into W-sapce with preprocessing and optimized noise Inputs. For z-space a mix of mse and vgg loss sometimes yields better results but in general only vgg loss yield comparable results.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ycZapQe863M"},"outputs":[],"source":["NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","DEVICE = 'cuda'\n","\n","def projection_z(img_path: Union[str, os.PathLike, BinaryIO, IO[bytes]],\n","                 num_epochs: int,\n","                 learning_rate,\n","                 save_path_projection: Union[str, os.PathLike, BinaryIO, IO[bytes]],\n","                 name,\n","                 save_path_aligned: Union[str, os.PathLike, BinaryIO, IO[bytes]] = None,\n","                 continue_training: bool = False,\n","                 pre_trained_image_path: Union[str, os.PathLike, BinaryIO, IO[bytes]] = None):\n","  \n","  if(save_path_aligned == None):\n","    target_img = preprocess_pipeline(img_path)\n","  else:\n","    target_img = preprocess_pipeline(img_path, True, save_path_aligned)\n","  \n","  target_img = np.asarray(target_img)\n","  target_img = torch.Tensor(target_img)\n","  target_img = target_img.view(1, 256, 256, 3).to(DEVICE)\n","  target_img = (target_img - 128) / 127.5\n","  target_img = target_img.permute(0, 3, 1, 2)\n","\n","\n","  plot_generator_img(target_img, \"Target Image\")\n","\n","  #Init Generator\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=DEVICE)\n","  \n","  truncation_psi = 0.7\n","  noise_mode = 'const'\n","\n","  #Init Noise\n","  noise_bufs = { name: buf for (name, buf) in G.synthesis.named_buffers() if 'noise_const' in name }\n","\n","\n","  #Init loss functions\n","  loss_fn_mse = torch.nn.MSELoss()\n","  loss_fn_vgg = VGGPerceptualLoss().to(DEVICE)\n","  \n","  #\n","  if(continue_training == False ):\n","      #Initiliza array of random image and use the best to project to\n","      z_list = [gen_rand_z(G) for i in range(2000)]\n","      z_img_list = [G(z, label,  truncation_psi=truncation_psi, noise_mode=noise_mode).to(DEVICE) for z in z_list]\n","\n","      #Calculate loss for each image\n","      z_img_error_list = [0.2* loss_fn_mse(z_img, target_img) + 0.8* loss_fn_vgg(z_img, target_img) for z_img in z_img_list]\n","\n","      #sort list in ascending order, after that the first element is the best\n","      z_list_sorted = sorted(zip(z_list, z_img_error_list), key=lambda x: x[1])\n","\n","      #copy best image to w\n","      z = z_list_sorted[0][0].clone().detach().requires_grad_(True)\n","\n","  else:\n","      #load pre trained image\n","      z = load_tensor(pre_trained_image_path).clone().detach().requires_grad_(True)\n","\n","  #initialize optimizer/scheduler\n","  learning_rate = learning_rate\n","  optimizer = torch.optim.Adam([z]+ list(noise_bufs.values()), lr=learning_rate)\n","\n","  #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n","  #scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=50, epochs=int(num_epochs/50))\n","\n","  for buf in noise_bufs.values():\n","    buf[:] = torch.randn_like(buf)\n","    buf.requires_grad = True\n","\n","  avg_loss = 0\n","  plot_loss = []\n","\n","  #Projection Step\n","  for epoch in range(num_epochs):\n","    optimizer.zero_grad()\n","    pred = G(z, label,  truncation_psi=truncation_psi, noise_mode=noise_mode)\n","\n","\n","    reg_loss = 0.0\n","    for v in noise_bufs.values():\n","        noise = v[None,None,:,:] # must be [1,1,H,W] for F.avg_pool2d()\n","        while True:\n","            reg_loss += (noise*torch.roll(noise, shifts=1, dims=3)).mean()**2\n","            reg_loss += (noise*torch.roll(noise, shifts=1, dims=2)).mean()**2\n","            if noise.shape[2] <= 8:\n","                break\n","            noise = F.avg_pool2d(noise, kernel_size=2)\n","    #Calculate loss\n","    loss_mse = loss_fn_mse(pred, target_img)\n","    loss_vgg = loss_fn_vgg(pred, target_img)\n","    loss = 0.2*loss_mse + 0.8*loss_vgg + 1e5 * reg_loss\n","    avg_loss += loss \n","\n","    #Optimize z\n","    loss.backward()\n","    optimizer.step()\n","\n","    \n","    #scheduler.step(loss)\n","\n","    #Print loss and current state of the projection\n","    if epoch % 100 == 0:\n","        print('Epoch: %d, avgloss: %f' % (epoch, avg_loss))\n","        print(loss_mse, loss_vgg)\n","        new_img = G(z, label,  truncation_psi=truncation_psi, noise_mode=noise_mode)\n","        plot_generator_img(new_img, \"Projection after \" +  str(epoch) +  \" epochs: \" )\n","        plot_loss.append(avg_loss)\n","        avg_loss = 0\n","        \n","    with torch.no_grad():\n","      for buf in noise_bufs.values():\n","        buf -= buf.mean()\n","        buf *= buf.square().mean().rsqrt()\n","\n","  plot_loss.append(avg_loss)\n","  plot_loss = [x.cpu().detach().numpy() for x in plot_loss]\n","  plot_loss = plot_loss[1:]\n","  plot_x = [x for  x in range(0, num_epochs+1, 100)]\n","  plot_x = plot_x[1:]\n","  plt.plot(plot_x, plot_loss)\n","  plt.xlabel(\"num_epochs\")\n","  plt.ylabel(\"avg_loss in last 100 epochs\")\n","  plt.title(\"Loss during training\")\n","  plt.show()\n","\n","\n","  final_img = G(z, label,  truncation_psi=truncation_psi, noise_mode=noise_mode)\n","  plot_generator_img(final_img, 'final for ' + str(num_epochs) + ' epochs using Adam Optimizer and Plateau schedule')\n","  plot_generator_img(target_img, 'Original Image')\n","\n","  save_tensor(z, save_path_projection + f'/z_tensors/{name}.pt')\n","  save_generator_img(final_img,  save_path_projection + f'/images_z/{name}.png')\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K9sl_pgAA96l"},"outputs":[],"source":["projection_z(f'/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/projection/target_images/blonde_frau.jpg',\n","            2000,\n","            0.002,\n","            \"/content/drive/MyDrive/Bachelorarbeit/ausarbeitung/gfx/projection\",\n","            f'blonde_frau',\n","            \"/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/projection/aligned_target_images/blonde_frau.jpg\")"]},{"cell_type":"markdown","metadata":{"id":"l33AayS4M5x1"},"source":["## Projection noise"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"23NkrT4eNAi9"},"outputs":[],"source":["NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","DEVICE = 'cuda'\n","\n","\n","\n","def projection_w_noise(img_path: Union[str, os.PathLike, BinaryIO, IO[bytes]],\n","                 num_epochs: int,\n","                 learning_rate,\n","                 save_path_projection: Union[str, os.PathLike, BinaryIO, IO[bytes]],\n","                 name,\n","                 save_path_aligned: Union[str, os.PathLike, BinaryIO, IO[bytes]] = None,\n","                 continue_training: bool = False,\n","                 pre_trained_image_path: Union[str, os.PathLike, BinaryIO, IO[bytes]] = None):\n","  \n","  if(save_path_aligned == None):\n","    target_img = preprocess_pipeline(img_path)\n","  else:\n","    target_img = preprocess_pipeline(img_path, True, save_path_aligned)\n","  \n","  target_img = np.asarray(target_img)\n","  target_img = torch.Tensor(target_img)\n","  target_img = target_img.view(1, 256, 256, 3).to(DEVICE)\n","  target_img = (target_img - 128) / 127.5\n","  target_img = target_img.permute(0, 3, 1, 2)\n","\n","  plot_generator_img(target_img, \"Target Image\")\n","\n","  #Init Generator\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=DEVICE)\n","  \n","  truncation_psi = 0.7\n","  noise_mode = 'const'\n","  N = 3*256*256\n","\n","  # Init noise.\n","  init_noise_factor = 0.05\n","  noise_bufs = { name: buf for (name, buf) in G.synthesis.named_buffers() if 'noise_const' in name }\n","\n","  #Init loss functions\n","  loss_fn_mse = torch.nn.MSELoss()\n","  loss_fn_vgg = VGGPerceptualLoss().to(DEVICE)\n","  \n","  #\n","  if(continue_training == False ):\n","      #Initiliza array of random image and use the best to project to\n","      w_list = [G.mapping(gen_rand_z(G), label) for i in range(2000)]\n","      w_img_list = [G.synthesis(w, noise_mode=noise_mode, force_fp32=False).to(DEVICE) for w in w_list]\n","\n","      #Calculate loss for each image\n","      w_img_error_list = [loss_fn_vgg(w_img, target_img) for w_img in w_img_list]\n","\n","      #sort list in ascending order, after that the first element is the best\n","      w_list_sorted = sorted(zip(w_list, w_img_error_list), key=lambda x: x[1])\n","\n","      #copy best image to w\n","      w = w_list_sorted[0][0].clone().detach().requires_grad_(True)\n","\n","  else:\n","      #load pre trained image\n","      w = load_tensor(pre_trained_image_path).clone().detach().requires_grad_(True)\n","\n","\n","  #w_samples_number = 10000\n","  #w_samples = [G.mapping(gen_rand_z(G), label).cpu().numpy().astype(np.float32)  for i in range(w_samples_number)]\n","  #w_avg = np.mean(w_samples, axis=0, keepdims=True)\n","  #w_std = (np.sum((w_samples - w_avg) ** 2) / w_samples_number) ** 0.5\n","\n","  #initialize optimizer/scheduler\n","  learning_rate = learning_rate\n","  optimizer = torch.optim.Adam([w] + list(noise_bufs.values()), betas=(0.9, 0.999), lr=learning_rate)\n","\n","  #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n","  #scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.995)\n","  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=50, epochs=int(num_epochs/50))\n","\n","  for buf in noise_bufs.values():\n","    buf[:] = torch.randn_like(buf)\n","    buf.requires_grad = True\n","\n","\n","  new_img = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","  plot_generator_img(new_img, \"Starting  Point \" )\n","\n","\n","  avg_loss = 0\n","  plot_loss = []\n","  plot_w = []\n","\n","  noise_ramp_length =  0.75\n","\n","  #Projection Step\n","  for epoch in range(num_epochs):\n","    optimizer.zero_grad()\n","    #Generate image from w\n","\n","    #t = epoch / num_epochs\n","    #noise_scale = w_std * init_noise_factor * max(0.0, 1.0 - t / noise_ramp_length) ** 2\n","    #w_noise = torch.randn_like(w) * noise_scale\n","    #print( noise_scale)\n","  \n","    #w_pred = (w + w_noise).repeat([1,1, 1])\n"," \n","\n","    pred = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","\n","    #regularise Noise, so that there is not semantic in it\n","    reg_loss = 0.0\n","    for v in noise_bufs.values():\n","        noise = v[None,None,:,:] # must be [1,1,H,W] for F.avg_pool2d()\n","        while True:\n","            reg_loss += (noise*torch.roll(noise, shifts=1, dims=3)).mean()**2\n","            reg_loss += (noise*torch.roll(noise, shifts=1, dims=2)).mean()**2\n","            if noise.shape[2] <= 8:\n","                break\n","            noise = F.avg_pool2d(noise, kernel_size=2)\n","\n","\n","\n","    #Calculate loss\n","    loss_mse = loss_fn_mse(pred, target_img)\n","    loss_vgg = loss_fn_vgg(pred, target_img)\n","    loss =  loss_vgg **2 + reg_loss * 1e5 + loss_mse *1/N\n","    avg_loss += loss \n","\n","    #Optimize w\n","    #optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if epoch %10 == 0:\n","      plot_w.append(w.clone().detach().cpu().numpy())\n","\n","    scheduler.step()\n","\n","    #Print loss and current state of the projection\n","    if epoch % 100 == 0:\n","        print('Epoch: %d, avgloss: %f' % (epoch, avg_loss))\n","        print(loss_mse, loss_vgg)\n","        new_img = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","        plot_generator_img(new_img, \"Projection after \" +  str(epoch) +  \" epochs: \" )\n","        plot_loss.append(avg_loss)\n","        avg_loss = 0\n","\n","    with torch.no_grad():\n","      for buf in noise_bufs.values():\n","        buf -= buf.mean()\n","        buf *= buf.square().mean().rsqrt()\n","\n","\n","\n","\n","  #plot_loss.append(avg_loss)\n","  #plot_loss = [x.cpu().detach().numpy() for x in plot_loss]\n","  #plot_loss = plot_loss[1:]\n","  #plot_x = [x for  x in range(0, num_epochs+1, 100)]\n","  #plot_x = plot_x[1:]\n","  #plt.plot(plot_x, plot_loss)\n","  #plt.xlabel(\"num_epochs\")\n","  #plt.ylabel(\"avg_loss in last 100 epochs\")\n","  #plt.title(\"Loss during training\")\n","  #plt.show()\n","\n","\n","  final_img = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","  plot_generator_img(final_img, 'final for ' + str(num_epochs) + ' epochs using Adam Optimizer and Plateau schedule')\n","  plot_generator_img(target_img, 'Original Bild')\n","\n","  save_tensor(w, save_path_projection + f'/w_tensors/{name}.pt')\n","  save_generator_img(final_img,  save_path_projection + f'/images/{name}.png')\n","\n","\n","  return plot_w\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g0uDiQ4CPBqA"},"outputs":[],"source":["\n","plot_w = projection_w_noise(f'/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/projection/target_images/paul.jpeg',\n","            2000,\n","            0.002,\n","            \"/content/drive/MyDrive/Bachelorarbeit/ausarbeitung/gfx/projection\",\n","            f'paul')\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mH0Q3uCA0LCp"},"outputs":[],"source":["\n","Perplexity = 30\n","\n","tsne = TSNE(n_components=3, verbose=1, random_state=123, perplexity=Perplexity)\n","embed = tsne.fit_transform(plot_w)\n","\n","x, y, z = list(zip(*embed))\n","\n","x = list(x)\n","y = list(y)\n","z = list(z)\n","\n","x_filtered = []\n","y_filtered = []\n","z_filtered = []\n","\n","print(len(x))\n","print(len(y))\n","print(len(z))\n","print(max(x),max(y),max(z))\n","print(min(x),min(y),min(z))\n","\n","indexes = []\n","#filter outliers\n","for i in range(len(x)):\n","  if abs(x[i]) < 22 and abs(y[i]) < 20 and abs(z[i]) < 20:\n","    x_filtered.append(x[i])\n","    y_filtered.append(y[i])\n","    z_filtered.append(z[i])\n","\n","n = len(x_filtered)\n","color = []\n","\n","for i in range(1, n+1):\n","  color.append(i)\n","\n","\n","# axes instance\n","fig = plt.figure(figsize=(6,6))\n","ax = Axes3D(fig, auto_add_to_figure=False)\n","fig.add_axes(ax)\n","\n","cmap = ListedColormap(sns.color_palette(\"blend:#F00,#00F\").as_hex())\n","\n","# plot\n","sc = ax.scatter(x_filtered, y_filtered, z_filtered, s=20, marker='o', alpha=1, c =color, cmap=cmap)\n","ax.set_xlabel('X')\n","ax.set_ylabel('Y')\n","ax.set_zlabel('Z')\n","ax.set_title(f'Perplexity: {Perplexity}')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"KccclMKvCUrO"},"source":["##Projection with noise 2nd\n","Includes some testing which dind't work out (different initalisation and adding noise to latent code to stabilise findding the global optimum). Further work may be done here. Current state is not compilable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CC6ucLdvCYM2"},"outputs":[],"source":["NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","DEVICE = 'cuda'\n","\n","def projection_w_noise(img_path: Union[str, os.PathLike, BinaryIO, IO[bytes]],\n","                 num_epochs: int,\n","                 learning_rate,\n","                 save_path_projection: Union[str, os.PathLike, BinaryIO, IO[bytes]],\n","                 name,\n","                 save_path_aligned: Union[str, os.PathLike, BinaryIO, IO[bytes]] = None,\n","                 continue_training: bool = False,\n","                 pre_trained_image_path: Union[str, os.PathLike, BinaryIO, IO[bytes]] = None):\n","  \n","  if(save_path_aligned == None):\n","    target_img = preprocess_pipeline(img_path)\n","  else:\n","    target_img = preprocess_pipeline(img_path, True, save_path_aligned)\n","  \n","  target_img = np.asarray(target_img)\n","  target_img = torch.Tensor(target_img)\n","  target_img = target_img.view(1, 256, 256, 3).to(DEVICE)\n","  target_img = (target_img - 128) / 127.5\n","  target_img = target_img.permute(0, 3, 1, 2)\n","\n","  print(target_img)\n","  plot_generator_img(target_img, \"Target Image\")\n","\n","  #Init Generator\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=DEVICE)\n","\n","  # Init noise.\n","  init_noise_factor = 0.05\n","  noise_bufs = { name: buf for (name, buf) in G.synthesis.named_buffers() if 'noise_const' in name }\n","\n","  truncation_psi = 0.7\n","  noise_mode = 'const'\n","\n","  #Init loss functions\n","  loss_fn_mse = torch.nn.MSELoss()\n","  loss_fn_vgg = VGGPerceptualLoss().to(DEVICE)\n","\n","  \n","  #\n","  if(continue_training == False ):\n","      #Initiliza array of random image and use the best to project to\n","      w_list = [G.mapping(gen_rand_z(G), label) for i in range(1000)]\n","      w_img_list = [G.synthesis(w, noise_mode=noise_mode, force_fp32=False).to(DEVICE) for w in w_list]\n","\n","      #Calculate loss for each image\n","      w_img_error_list = [loss_fn_vgg(w_img, target_img) for w_img in w_img_list]\n","\n","      #sort list in ascending order, after that the first element is the best\n","      w_list_sorted = sorted(zip(w_list, w_img_error_list), key=lambda x: x[1])\n","\n","      #copy best image to w\n","      w = w_list_sorted[0][0].clone().detach().requires_grad_(True)\n","\n","  else:\n","      #load pre trained image\n","      w = load_tensor(pre_trained_image_path).clone().detach().requires_grad_(True)\n","\n","\n","  num_samples = 10000\n","  w_samples = [G.mapping(gen_rand_z(G), label).cpu().numpy().astype(np.float32)  for i in range(num_samples)]\n","  w_avg = np.mean(w_samples, axis=0, keepdims=True)\n","  w_std = (np.sum((w_samples - w_avg) ** 2) / num_samples) ** 0.5\n","\n","  #initialize optimizer/scheduler\n","  learning_rate = learning_rate\n","  optimizer = torch.optim.Adam([w] + list(noise_bufs.values()), betas=(0.9, 0.999), lr=learning_rate)\n","\n","  for buf in noise_bufs.values():\n","      buf[:] = torch.randn_like(buf)\n","      buf.requires_grad = True\n","\n","  #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n","  #scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.995)\n","  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=50, epochs=int(num_epochs/50))\n","\n","\n","  new_img = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","  plot_generator_img(new_img, \"Starting  Point \" )\n","\n","\n","  avg_loss = 0\n","  plot_loss = []\n","\n","  three_quartes = num_epochs * 3 /4\n","\n","  #Projection Step\n","  for epoch in range(num_epochs):\n","\n","    factor = max(0, 1 - (epoch / three_quartes))\n","    noise_scale = init_noise_factor * w_std * (factor ** 2)\n","    w_noise = torch.randn_like(w) * noise_scale\n","    w = (w + w_noise)\n","    optimizer.zero_grad()\n","    pred = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","\n","    #Generate image from w\n","    #pred = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","    #Calculate loss\n","\n","\n","    #regularise Noise, so that there is not semantic in it\n","    reg_loss = 0.0\n","    for v in noise_bufs.values():\n","        noise = v[None,None,:,:] # must be [1,1,H,W] for F.avg_pool2d()\n","        while True:\n","            reg_loss += (noise*torch.roll(noise, shifts=1, dims=3)).mean()**2\n","            reg_loss += (noise*torch.roll(noise, shifts=1, dims=2)).mean()**2\n","            if noise.shape[2] <= 8:\n","                break\n","            noise = F.avg_pool2d(noise, kernel_size=2)\n","\n","    loss_mse = loss_fn_mse(pred, target_img)\n","    loss_vgg = loss_fn_vgg(pred, target_img)\n","    loss = loss_vgg ** 2  + reg_loss * 1e6\n","    #loss = 0.2 * loss_mse + 0.8* loss_vgg  \n","    avg_loss += loss \n","\n","    #Optimize w\n","    #optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    #if epoch %10 == 0:\n","    scheduler.step()\n","\n","\n","    #Print loss and current state of the projection\n","    if epoch % 100 == 0:\n","        print('Epoch: %d, avgloss: %f' % (epoch, avg_loss))\n","        print(loss_mse, loss_vgg)\n","        new_img = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","        plot_generator_img(new_img, \"Projection after \" +  str(epoch) +  \" epochs: \" )\n","        plot_loss.append(avg_loss)\n","        avg_loss = 0\n","\n","    with torch.no_grad():\n","      for buf in noise_bufs.values():\n","        buf -= buf.mean()\n","        buf *= buf.square().mean().rsqrt()\n","\n","  plot_loss.append(avg_loss)\n","  plot_loss = [x.cpu().detach().numpy() for x in plot_loss]\n","  plot_loss = plot_loss[1:]\n","  plot_x = [x for  x in range(0, num_epochs+1, 100)]\n","  plot_x = plot_x[1:]\n","  plt.plot(plot_x, plot_loss)\n","  plt.xlabel(\"num_epochs\")\n","  plt.ylabel(\"avg_loss in last 100 epochs\")\n","  plt.title(\"Loss during training\")\n","  plt.show()\n","\n","\n","  final_img = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","  plot_generator_img(final_img, 'final for ' + str(num_epochs) + ' epochs using Adam Optimizer and Plateau schedule')\n","  plot_generator_img(target_img, 'Original Image')\n","\n","  save_tensor(w, save_path_projection + f'/w_tensors/{name}.pt')\n","  save_generator_img(final_img,  save_path_projection + f'/images/{name}.png')\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S8hW97bICid3"},"outputs":[],"source":["projection_w_noise(\"/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/projection/target_images/til.jpg\",\n","            1000,\n","            0.002,\n","            \"/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/projection/result_images\",\n","            'test3')"]},{"cell_type":"markdown","metadata":{"id":"52t2_dAh79T9"},"source":["##Projection with noise\n","Another experimental version which doesn't compile."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uVO3Zcz08Wp8"},"outputs":[],"source":["NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","DEVICE = 'cuda'\n","\n","def projection_w(img_path: Union[str, os.PathLike, BinaryIO, IO[bytes]],\n","                 num_epochs: int,\n","                 learning_rate,\n","                 save_path_projection: Union[str, os.PathLike, BinaryIO, IO[bytes]],\n","                 name,\n","                 save_path_aligned: Union[str, os.PathLike, BinaryIO, IO[bytes]] = None,\n","                 continue_training: bool = False,\n","                 pre_trained_image_path: Union[str, os.PathLike, BinaryIO, IO[bytes]] = None):\n","  \n","  if(save_path_aligned == None):\n","    target_img = preprocess_pipeline(img_path)\n","  else:\n","    target_img = preprocess_pipeline(img_path, True, save_path_aligned)\n","  \n","  target_img = np.asarray(target_img)\n","  target_img = torch.Tensor(target_img)\n","  target_img = target_img.view(1, 256, 256, 3).to(DEVICE)\n","  target_img = (target_img - 128) / 127.5\n","  target_img = target_img.permute(0, 3, 1, 2)\n","\n","  plot_generator_img(target_img, \"Target Image\")\n","\n","  #Init Generator\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=DEVICE)\n","\n","  # Init noise.\n","  init_noise_factor = 0.05\n","  noise_bufs = { name: buf for (name, buf) in G.synthesis.named_buffers() if 'noise_const' in name }\n","\n","  #for buf in noise_bufs.values():\n","  #    buf[:] = torch.randn_like(buf)\n","  #    buf.requires_grad = True\n","\n","  torch.cuda.memory_summary(device=None, abbreviated=False)\n","  \n","  truncation_psi = 0.7\n","  noise_mode = 'const'\n","\n","  #Init loss functions\n","  loss_fn_mse = torch.nn.MSELoss()\n","  loss_fn_vgg = VGGPerceptualLoss().to(DEVICE)\n","  \n","  #\n","  if(continue_training == False ):\n","      #Initiliza array of random image and use the best to project to\n","      w_list = [G.mapping(gen_rand_z(G), label) for i in range(2000)]\n","      w_img_list = [G.synthesis(w, noise_mode=noise_mode, force_fp32=False).to(DEVICE) for w in w_list]\n","\n","      #Calculate loss for each image\n","      w_img_error_list = [loss_fn_mse(w_img, target_img) + 0.8 * loss_fn_vgg(w_img, target_img) for w_img in w_img_list]\n","\n","      #\n","  \n","\n","      #sort list in ascending order, after that the first element is the best\n","      w_list_sorted = sorted(zip(w_list, w_img_error_list), key=lambda x: x[1])\n","\n","      #copy best image to w\n","      w = w_list_sorted[0][0].clone().detach().requires_grad_(True)\n","\n","      #w_list = [G.mapping(gen_rand_z(G), label).cpu().numpy().astype(np.float32)  for i in range(2)]\n","\n","      #w_tensor = torch.stack(w_list)\n","      #w_avg = torch.mean(w_tensor, 0)\n","      #w_avg = np.mean(w_list, axis=0, keepdims=True)\n","      #w_std = (np.sum((w_list - w_avg) ** 2) / 10) ** 0.5\n","\n","\n","  else:\n","      #load pre trained image\n","      w = load_tensor(pre_trained_image_path).clone().detach().requires_grad_(True)\n","\n","  w_list = w_list.cpu().numpy().astype(np.float32)\n","  w_avg = np.mean(w_list, axis=0, keepdims=True)\n","  w_std = (np.sum((w_list - w_avg) ** 2) / 10) ** 0.5\n","\n","\n","\n","  #initialize optimizer/scheduler\n","  learning_rate = learning_rate\n","  optimizer = torch.optim.Adam([w] + list(noise_bufs.values()), betas = (0.9,0.999), lr=learning_rate)\n","\n","  #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n","  #scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.995)\n","  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=50, epochs=int(num_epochs/50))\n","\n","\n","  new_img = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","  plot_generator_img(new_img, \"Starting  Point \" )\n","\n","\n","  avg_loss = 0\n","  plot_loss = []\n","\n","  three_quartes = num_epochs * 3 /4\n","\n","  #Projection Step\n","  for epoch in range(num_epochs):\n","    factor = max(0, 1 - (num_epochs / three_quartes))\n","    noise_scale = init_noise_factor * w_std * factor ** 2\n","    w_noise = torch.randn_like(w) * noise_scale\n","    optimizer.zero_grad()\n","    w = (w + w_noise).repeat([1, G.mapping.num_ws, 1])\n","    pred = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","    #Generate image from w\n","    pred = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","    #Calculate loss\n","    loss_mse = loss_fn_mse(pred, target_img)\n","    loss_vgg = loss_fn_vgg(pred, target_img)\n","    loss = 0.2 * loss_mse + 0.8* loss_vgg\n","    avg_loss += loss \n","\n","    #Optimize w\n","    #optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    #if epoch %10 == 0:\n","    scheduler.step()\n","\n","    #Print loss and current state of the projection\n","    if epoch % 100 == 0:\n","        print('Epoch: %d, avgloss: %f' % (epoch, avg_loss))\n","        print(loss_mse, loss_vgg)\n","        new_img = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","        plot_generator_img(new_img, \"Projection after \" +  str(epoch) +  \" epochs: \" )\n","        plot_loss.append(avg_loss)\n","        avg_loss = 0\n","\n","  plot_loss.append(avg_loss)\n","  plot_loss = [x.cpu().detach().numpy() for x in plot_loss]\n","  plot_loss = plot_loss[1:]\n","  plot_x = [x for  x in range(0, num_epochs+1, 100)]\n","  plot_x = plot_x[1:]\n","  plt.plot(plot_x, plot_loss)\n","  plt.xlabel(\"num_epochs\")\n","  plt.ylabel(\"avg_loss in last 100 epochs\")\n","  plt.title(\"Loss during training\")\n","  plt.show()\n","\n","\n","  final_img = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","  plot_generator_img(final_img, 'final for ' + str(num_epochs) + ' epochs using Adam Optimizer and Plateau schedule')\n","  plot_generator_img(target_img, 'Original Image')\n","\n","  save_tensor(w, save_path_projection + f'/w_tensors/{name}.pt')\n","  save_generator_img(final_img,  save_path_projection + f'/images/{name}.png')\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fz4EDx7B8nVX"},"outputs":[],"source":["projection_w(\"/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/projection/target_images/til.jpg\",\n","            2000,\n","            0.002,\n","            \"/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/projection/result_images\",\n","            'til_with_noise',\n","            \"/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/projection/aligned_target_images/til2.jpg\")"]},{"cell_type":"markdown","source":["## Projection for non face images into ffhq\n","Used in Section 3.3.4 to embedd non facial images in the latent space of the generator trained on ffhq. Only use 256x256 Images ff there is no recognizable face in the image, otherwise any image is possible "],"metadata":{"id":"d2jypvirzX64"}},{"cell_type":"code","source":["NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","DEVICE = 'cuda'\n","\n","\n","\n","def projection_w_noise_limit(img_path: Union[str, os.PathLike, BinaryIO, IO[bytes]],\n","                 num_epochs: int,\n","                 learning_rate,\n","                 save_path_projection: Union[str, os.PathLike, BinaryIO, IO[bytes]],\n","                 name,\n","                 save_path_aligned: Union[str, os.PathLike, BinaryIO, IO[bytes]] = None,\n","                 continue_training: bool = False,\n","                 pre_trained_image_path: Union[str, os.PathLike, BinaryIO, IO[bytes]] = None):\n","  \n","  #Uncommend if there is an recognizable face in the image.\n","  #if(save_path_aligned == None):\n","  #  target_img = preprocess_pipeline(img_path)\n","  #else:\n","  #  target_img = preprocess_pipeline(img_path, True, save_path_aligned)\n","  \n","  #target_img = np.asarray(target_img)\n","  #target_img = torch.Tensor(target_img)\n","  #target_img = target_img.view(1, 256, 256, 3).to(DEVICE)\n","  #target_img = (target_img - 128) / 127.5\n","  #target_img = target_img.permute(0, 3, 1, 2)\n","\n","  #plot_generator_img(target_img, \"Target Image\")\n","\n","  target_img = prepare_image(img_path, 256)\n","  plot_generator_img(target_img, \"Target Image\")\n","\n","  #Init Generator\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=DEVICE)\n","  \n","  truncation_psi = 0.7\n","  noise_mode = 'const'\n","\n","  # Init noise.\n","  init_noise_factor = 0.05\n","  noise_bufs = { name: buf for (name, buf) in G.synthesis.named_buffers() if 'noise_const' in name }\n","\n","  #Init loss functions\n","  loss_fn_mse = torch.nn.MSELoss()\n","  loss_fn_vgg = VGGPerceptualLoss().to(DEVICE)\n","  \n","  #\n","  if(continue_training == False ):\n","      #Initiliza array of random image and use the best to project to\n","      w_list = [G.mapping(gen_rand_z(G), label) for i in range(2000)]\n","      w_img_list = [G.synthesis(w, noise_mode=noise_mode, force_fp32=False).to(DEVICE) for w in w_list]\n","\n","      #Calculate loss for each image\n","      w_img_error_list = [loss_fn_vgg(w_img, target_img) for w_img in w_img_list]\n","\n","      #sort list in ascending order, after that the first element is the best\n","      w_list_sorted = sorted(zip(w_list, w_img_error_list), key=lambda x: x[1])\n","\n","      #copy best image to w\n","      w = w_list_sorted[0][0].clone().detach().requires_grad_(True)\n","\n","  else:\n","      #load pre trained image\n","      w = load_tensor(pre_trained_image_path).clone().detach().requires_grad_(True)\n","\n","  # Used for experimental approach, which didn't work out.\n","  #w_samples_number = 10000\n","  #w_samples = [G.mapping(gen_rand_z(G), label).cpu().numpy().astype(np.float32)  for i in range(w_samples_number)]\n","  #w_avg = np.mean(w_samples, axis=0, keepdims=True)\n","  #w_std = (np.sum((w_samples - w_avg) ** 2) / w_samples_number) ** 0.5\n","\n","  #initialize optimizer/scheduler\n","  learning_rate = learning_rate\n","  optimizer = torch.optim.Adam([w], betas=(0.9, 0.999), lr=learning_rate)\n","\n","  #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n","  #scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.995)\n","  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=50, epochs=int(num_epochs/50))\n","\n","  for buf in noise_bufs.values():\n","    buf[:] = torch.randn_like(buf)\n","    buf.requires_grad = True\n","\n","\n","  new_img = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","  plot_generator_img(new_img, \"Starting  Point \" )\n","\n","\n","  avg_loss = 0\n","  plot_loss = []\n","  plot_w = []\n","\n","  noise_ramp_length =  0.75\n","\n","  #Projection Step\n","  for epoch in range(num_epochs):\n","    optimizer.zero_grad()\n","    #Generate image from w\n","\n","    #experimental approach, which didn't work out.\n","    #t = epoch / num_epochs\n","    #noise_scale = w_std * init_noise_factor * max(0.0, 1.0 - t / noise_ramp_length) ** 2\n","    #w_noise = torch.randn_like(w) * noise_scale\n","    #print( noise_scale)\n","  \n","    #w_pred = (w + w_noise).repeat([1,1, 1])\n"," \n","\n","    pred = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","\n","    #Experiments show that even a high reugularisation results semantic in the noise Inputs. Only uncomment for testing.\n","    #regularise Noise, so that there is not semantic in it\n","    #reg_loss = 0.0\n","    #for v in noise_bufs.values():\n","    #    noise = v[None,None,:,:] # must be [1,1,H,W] for F.avg_pool2d()\n","    #    while True:\n","    #        reg_loss += (noise*torch.roll(noise, shifts=1, dims=3)).mean()**2\n","    #        reg_loss += (noise*torch.roll(noise, shifts=1, dims=2)).mean()**2\n","    #        if noise.shape[2] <= 8:\n","    #            break\n","    #        noise = F.avg_pool2d(noise, kernel_size=2)\n","\n","\n","\n","    #Calculate loss\n","    loss_mse = loss_fn_mse(pred, target_img)\n","    loss_vgg = loss_fn_vgg(pred, target_img)\n","    loss =  loss_vgg **2  + loss_mse *1/N\n","    avg_loss += loss \n","\n","    #Optimize w\n","    #optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if epoch %10 == 0:\n","      plot_w.append(w.clone().detach().cpu().numpy())\n","\n","    scheduler.step()\n","\n","    #Print loss and current state of the projection\n","    if epoch % 100 == 0:\n","        print('Epoch: %d, avgloss: %f' % (epoch, avg_loss))\n","        print(loss_mse, loss_vgg)\n","        new_img = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","        plot_generator_img(new_img, \"Projection after \" +  str(epoch) +  \" epochs: \" )\n","        plot_loss.append(avg_loss)\n","        avg_loss = 0\n","\n","    #dont use noise inputs\n","    #with torch.no_grad():\n","    #  for buf in noise_bufs.values():\n","    #    buf -= buf.mean()\n","    #    buf *= buf.square().mean().rsqrt()\n","\n","\n","\n","  #Uncomment if you want plots of the loss value during projection.\n","  #plot_loss.append(avg_loss)\n","  #plot_loss = [x.cpu().detach().numpy() for x in plot_loss]\n","  #plot_loss = plot_loss[1:]\n","  #plot_x = [x for  x in range(0, num_epochs+1, 100)]\n","  #plot_x = plot_x[1:]\n","  #plt.plot(plot_x, plot_loss)\n","  #plt.xlabel(\"num_epochs\")\n","  #plt.ylabel(\"avg_loss in last 100 epochs\")\n","  #plt.title(\"Loss during training\")\n","  #plt.show()\n","\n","\n","  final_img = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","  plot_generator_img(final_img, 'final for ' + str(num_epochs) + ' epochs using Adam Optimizer and Plateau schedule')\n","  plot_generator_img(target_img, 'Original Bild')\n","\n","  save_tensor(w, save_path_projection + f'/w_tensors/{name}.pt')\n","  save_generator_img(final_img,  save_path_projection + f'/images/{name}.png')\n","\n","\n","  return plot_w\n","\n","\n"],"metadata":{"id":"Q8JAlZSEzdAQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","plot_w = projection_w_noise_limit(f'/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/projection/target_images/Zimmer.jpg',\n","            3000,\n","            0.002,\n","            \"/content/drive/MyDrive/Bachelorarbeit/ausarbeitung/gfx/projection\",\n","            f'zimmer2')\n","\n","\n"],"metadata":{"id":"jAlimX2XzhYj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QNarqgvPHPRI"},"source":["#Feature Vectors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p8xINqpNHSg5"},"outputs":[],"source":["NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","DEVICE = 'cuda'\n","\n","#Reference: https://github.com/rosinality/stylegan2-pytorch/blob/master/closed_form_factorization.py\n","\n","def generate_eigvec(pkl):\n","\n","  device = torch.device(DEVICE)\n","  with dnnlib.util.open_url(pkl) as f:\n","    G = legacy.load_network_pkl(f)['G_ema'].to(device)\n","\n","  #Gewichte aus dem Affinen Transformationen \n","  M = {\n","    k[0]: k[1]\n","    for k in G.named_parameters()\n","    if \"affine\" in k[0] and \"torgb\" not in k[0] and \"weight\" in k[0] or (\"torgb\" in k[0] and \"b4\" in k[0] and \"weight\" in k[0] and \"affine\" in k[0])\n","  }\n","\n","\n","\n","  weight_mat = []\n","  for k, v in M.items():\n","    weight_mat.append(v)\n","\n","\n","  #Stack alle affinen Gewichtsmatrizen\n","  W = torch.cat(weight_mat, 0)\n","  #Berechnen der Eigenwerte mittels svd\n","  eigvec = torch.linalg.svd(W).Vh.to(\"cpu\")\n","\n","\n","\n","  return eigvec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjJy1QCiIeyA"},"outputs":[],"source":["eigvec = generate_eigvec(NETWORK_PKL)\n","print(eigvec.shape)"]},{"cell_type":"markdown","metadata":{"id":"h9FUhowWKLLZ"},"source":["Now we experiment with the computed feature vectors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AjkG9MEaKP7f"},"outputs":[],"source":["NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","DEVICE = 'cuda'\n","\n","def add_feature_vector_to_image(seed, degree, feature_vector_index, steps):\n","\n","  #Number of steps must be uneven\n","  if(steps % 2 == 0):\n","    steps = steps + 1\n","\n","  truncation_psi = 0.7\n","  noise_mode = 'const'  \n","\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=DEVICE)\n","\n","  z = np.random.RandomState(seed).randn(1, G.z_dim)\n","  z = torch.from_numpy(z)\n","\n","  w = G.mapping(torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE), label, truncation_psi=truncation_psi).cpu()\n"," \n","\n","  #just for clarification\n","  degree = degree # Shoud be between 0 and +-10, with 0 doing nothing \n","  vector_index = feature_vector_index # any number 0-511 (eig.vec.shape[0]) \n","\n","  eigvec = generate_eigvec(NETWORK_PKL)\n","  cur_eigvec = eigvec[vector_index]\n","  direction = degree * cur_eigvec\n","\n","  ws = []\n","  \n","\n","  for i in range(steps//2):\n","    ws.append(w-(direction*(1-(i/(steps//2)))))\n","  \n","  ws.append(w)\n","\n","  for i in range(steps//2):\n","    ws.append(w+(direction*(1-(i/(steps//2)))))\n","\n"," \n","\n","  ws = torch.cat([w for w in ws]).to(DEVICE)\n","\n","  \n","\n","  img = G.synthesis(ws, noise_mode=noise_mode, force_fp32=False)\n","  img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","\n","\n","  imshow(img.cpu(), col=steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2UCDVLS3KZWJ"},"outputs":[],"source":["\n","for i in range(10):\n","    add_feature_vector_to_image(175, 3, i+2, 5)"]},{"cell_type":"markdown","metadata":{"id":"AqwSN5RbLyki"},"source":["#Style Transfer"]},{"cell_type":"markdown","metadata":{"id":"8i9mb1C-NoFQ"},"source":["## Switching Styleblocks\n","Implementation of Style Mixing for two generated Images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cE9ANwgSMCtj"},"outputs":[],"source":["NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","DEVICE = 'cuda'\n","\n","def switch_styleblocks(seed1, seed2, indexes, i , j):\n","\n","  truncation_psi = 0.7\n","  noise_mode = 'const'\n","\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=DEVICE)\n","\n","  z1 = torch.from_numpy(np.random.RandomState(seed1).randn(1, G.z_dim)).to(DEVICE)\n","  z2 = torch.from_numpy(np.random.RandomState(seed2).randn(1, G.z_dim)).to(DEVICE)  \n","\n","  w1 = G.mapping(z1, label, truncation_psi=truncation_psi, truncation_cutoff=8)\n","  w2 = G.mapping(z2, label, truncation_psi=truncation_psi, truncation_cutoff=8)\n","\n","  w_mixed = w1.clone()\n","\n","  for index in indexes:\n","    w_mixed[:, index, :] = w2[:, index, :]\n","\n","  ws = [w1, w_mixed, w2]\n","\n","  ws = torch.cat([w for w in ws]).to(DEVICE)\n","\n","  img = G.synthesis(ws, noise_mode=noise_mode, force_fp32=False)\n","\n","  img_mixed = img[1].clone()\n","\n","  img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","\n","  #for w in ws:\n","  #  img = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","  #  images.append(img)\n","\n","  img_mixed = img_mixed.reshape((1, 3,256,256))\n","\n","  #plot_generator_img(img_mixed, \"mixed image\")\n","  save_generator_img(img_mixed,  f'/content/drive/MyDrive/Bachelorarbeit/ausarbeitung/gfx/style_mixing/mixed_bilder/8-13/Bild{i}{j}.png')\n","\n","  #imshow(img.cpu(), len(img))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p3flCUeWM-DB"},"outputs":[],"source":["for i in range(5):\n","  for j in range(5):\n","    switch_styleblocks(1300+i, 1500+j, [8,9,10,11,12,13], i, j)"]},{"cell_type":"markdown","metadata":{"id":"zkja-EexDxOc"},"source":["##Switching Styleblocks Tensor\n","Implementation of Style Mixing for one generated image and one image from our projection (latent code already computed)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hFcQI0R3In5E"},"outputs":[],"source":["NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","DEVICE = 'cuda'\n","\n","def switch_styleblocks_custom(seed, tensor_path, indexes):\n","\n","  truncation_psi = 0.7\n","  noise_mode = 'const'\n","\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=DEVICE)\n","\n","\n","  z1 = torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE)  \n","\n","  w2 = load_tensor(tensor_path)\n","  w1 = G.mapping(z1, label, truncation_psi=truncation_psi, truncation_cutoff=8)\n","\n","  w_mixed = w1.clone()\n","\n","  for index in indexes:\n","    w_mixed[:, index, :] = w2[:, index, :]\n","\n","  ws = [w1, w_mixed, w2]\n","\n","  ws = torch.cat([w for w in ws]).to(DEVICE)\n","\n","  img = G.synthesis(ws, noise_mode=noise_mode, force_fp32=False)\n","\n","  img_mixed = img[1].clone()\n","\n","  img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","\n","  #for w in ws:\n","  #  img = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","  #  images.append(img)\n","\n","  img_mixed = img_mixed.reshape((1, 3,256,256))\n","\n","  plot_generator_img(img_mixed,\"mixed image\")\n","  save_generator_img(img_mixed,  f'/content/drive/MyDrive/Bachelorarbeit/ausarbeitung/gfx/style_mixing/mixed_bilder/0-3/Bild{i}{j}.pt')\n","\n","  imshow(img.cpu(), len(img))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mPIsW7ceJA5M"},"outputs":[],"source":["\"/content/drive/MyDrive/Bachelorarbeit/ausarbeitung/gfx/projection/w_tensors/macron_with_noise.pt\"\n","\n","for i in range(820,830,1): \n","  switch_styleblocks_custom(i, '/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/projection/result_images/w_tensors/hendric_onecycle_high_0.002.pt', [0,1,2,3])\n","  #switch_styleblocks_custom(i, \"/content/drive/MyDrive/Bachelorarbeit/ausarbeitung/gfx/projection/w_tensors/blonde_frau.pt\", [4,5,6,7])"]},{"cell_type":"markdown","metadata":{"id":"TA1ZnG5INRXh"},"source":["##Transfer Style (Single)\n","First computes the latent Code embedding for the given Image and then computes the style mixing between the projected Image and the generated image by the given seed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ucLNi2KOK9g"},"outputs":[],"source":["NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","DEVICE = 'cuda'\n","\n","def style_transfer_single(img1_target_path, img1_save_path, img1_aligned_target_save_path, img1_name, epochs, result_save_path, indexes, seed):\n","\n","  projection_w(img1_target_path,\n","             epochs,\n","             0.02,\n","             img1_save_path,\n","             img1_name,\n","             img1_aligned_target_save_path)\n","  \n","\n","  truncation_psi = 0.7\n","  noise_mode = 'const'\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=DEVICE)\n","\n","  z1 = torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE)  \n","  w1 = G.mapping(z1, label, truncation_psi=truncation_psi, truncation_cutoff=8)\n","\n","  w2 =  load_tensor(img1_save_path + f'/w_tensors/{img1_name}.pt')\n","\n","  w_mixed = w1.clone()\n","\n","  for index in indexes:\n","    w_mixed[:, index, :] = w2[:, index, :]\n","  \n","  ws = [w1, w_mixed, w2]\n","\n","  ws = torch.cat([w for w in ws]).to(DEVICE)\n","\n","  img = G.synthesis(ws, noise_mode=noise_mode, force_fp32=False)\n","\n","  img_mixed = img[1].clone()\n","  img_mixed = img_mixed.reshape((1, 3,256,256))\n","\n","  img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","\n","  #for w in ws:\n","  #  img = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","  #  images.append(img)\n","\n","  imshow(img.cpu(), len(img))\n","  plot_generator_img(img_mixed, 'mixed image')\n","  save_generator_img(img_mixed, result_save_path)\n","\n","\n","\n","  \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RkwkDJyBPGxk"},"outputs":[],"source":["style_transfer_single(\"/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/style-transfer/target_images/hemsworth.jpg\",\n","               \"/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/style-transfer/projection_results\",\n","               \"/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/style-transfer/aligned_target_images/hemsworth_single_4.jpg\",\n","               \"hemsworth_single_5\",\n","               2000,\n","               \"/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/style-transfer/result_images/hemsworth_seed3000_single_0123_optimized.jpg\",\n","               [0,1,2,3],\n","               3000)\n"]},{"cell_type":"markdown","metadata":{"id":"-KYn1rcDNtkG"},"source":["## Transfer Style (Double)\n","First computes the latent Code embedding for two given Images and then computes the style mixing between the projected images. Very computational expensive and needs a lot of projektion epochs for stable results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MERCjJEwN5y-"},"outputs":[],"source":["NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","DEVICE = 'cuda'\n","\n","def style_transfer(img1_target_path, img1_save_path, img1_aligned_target_save_path, img1_name, img2_target_path, img2_save_path, img2_aligned_target_save_path, img2_name, epochs, result_save_path, indexes):\n","\n","  projection_w(img1_target_path,\n","             epochs,\n","             0.02,\n","             img1_save_path,\n","             img1_name,\n","             img1_aligned_target_save_path)\n","  \n","  projection_w(img2_target_path,\n","             epochs,\n","             0.02,\n","             img2_save_path,\n","             img2_name,\n","             img2_aligned_target_save_path)\n","  \n","  w1 =  load_tensor(img1_save_path + f'/w_tensors/{img1_name}.pt')\n","  w2 =  load_tensor(img2_save_path + f'/w_tensors/{img2_name}.pt')\n","\n","  \n","  truncation_psi = 0.7\n","  noise_mode = 'const'\n","\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=DEVICE)\n","\n","  w_mixed = w1.clone()\n","\n","  for index in indexes:\n","    w_mixed[:, index, :] = w2[:, index, :]\n","  \n","  ws = [w1, w_mixed, w2]\n","\n","  ws = torch.cat([w for w in ws]).to(DEVICE)\n","\n","  img = G.synthesis(ws, noise_mode=noise_mode, force_fp32=False)\n","\n","  img_mixed = img[1].clone()\n","  img_mixed = img_mixed.reshape((1, 3,256,256))\n","\n","  img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","\n","  #for w in ws:\n","  #  img = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","  #  images.append(img)\n","\n","  imshow(img.cpu(), len(img))\n","  plot_generator_img(img_mixed, 'mixed image')\n","  save_generator_img(img_mixed, result_save_path)\n","  \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"El5E2YtnUTYd"},"outputs":[],"source":["style_transfer(\"/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/style-transfer/target_images/macron.jpg\",\n","               \"/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/style-transfer/projection_results\",\n","               \"/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/style-transfer/aligned_target_images/macron.jpg\",\n","               \"macron_high_3\",\n","               \"/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/style-transfer/target_images/hemsworth.jpg\",\n","               \"/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/style-transfer/projection_results\",\n","               \"/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/style-transfer/aligned_target_images/hemsworth.jpg\",\n","               \"hemsworth_high_3\",\n","               3000,\n","               \"/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/style-transfer/result_images/hemsworth-macron_high_3_01234.jpg\",\n","               [0,1,2,3])\n"]},{"cell_type":"markdown","metadata":{"id":"qw3NoaG7fSBr"},"source":["## Tansfer Style (Double) Tensor\n","Style Mixing between two alredy projected images. Projection needs a lot of epochs for good results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_HsmquFQerCf"},"outputs":[],"source":["def style_transfer_tensor(img1_path, img2_path, result_save_path, indexes):\n","\n","\n","  \n","  w1 =  load_tensor(img1_path)\n","  w2 =  load_tensor(img2_path)\n","\n","  \n","  truncation_psi = 0.7\n","  noise_mode = 'const'\n","\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=DEVICE)\n","\n","  w_mixed = w1.clone()\n","\n","  for index in indexes:\n","    w_mixed[:, index, :] = w2[:, index, :]\n","  \n","  ws = [w1, w_mixed, w2]\n","\n","  ws = torch.cat([w for w in ws]).to(DEVICE)\n","\n","  img = G.synthesis(ws, noise_mode=noise_mode, force_fp32=False)\n","\n","  img_mixed = img[1].clone()\n","  img_mixed = img_mixed.reshape((1, 3,256,256))\n","\n","  img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","\n","  #for w in ws:\n","  #  img = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","  #  images.append(img)\n","\n","  imshow(img.cpu(), len(img))\n","  plot_generator_img(img_mixed, 'mixed image')\n","  save_generator_img(img_mixed, result_save_path)\n","  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k-7ic9-Sfehm"},"outputs":[],"source":["style_transfer_tensor('/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/projection/result_images/w_tensors/hendric_noise.pt',\n","               '/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/projection/result_images/w_tensors/til_noise.pt',\n","               \"/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/style-transfer/result_images/hendric_mona_high.jpg\",\n","               [0,1,2])"]},{"cell_type":"markdown","metadata":{"id":"nw7iarZdriIi"},"source":["#Testing\n","Testing different functionalities."]},{"cell_type":"markdown","metadata":{"id":"1CMnD_4hrmEL"},"source":["##Loading and saving"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t3-FD49zqELL"},"outputs":[],"source":["#Generate a random image, display image, save image, load image and display again\n","NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","Path = \"/content/drive/MyDrive/Bachelorarbeit/bachelorarbeit/testing\"\n","def load_save_test_z():\n","\n","  device = 'cuda'\n","  truncation_psi = 0.7\n","  noise_mode = 'const'\n","\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=device)\n","\n","  z = gen_rand_z(G)\n","\n","  img = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","\n","  plot_generator_img(img, 'Image to Save')\n","\n","  save_tensor(z, Path + '/test.pt')\n","  z_load = load_tensor(Path + '/test.pt')\n","\n","  load_img = G(z_load, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","\n","  plot_generator_img(load_img, \"Reloaded image\")\n","\n","\n","def load_save_test_w():\n","\n","  device = 'cuda'\n","  truncation_psi = 0.7\n","  noise_mode = 'const'\n","\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=device)\n","\n","  z = gen_rand_z(G)\n","  w = G.mapping(z, label, truncation_psi=truncation_psi, truncation_cutoff=8)\n","\n","  img = G.synthesis(w, noise_mode=noise_mode, force_fp32=False)\n","\n","  plot_generator_img(img, 'Image to Save')\n","\n","  save_tensor(w, Path + '/test.pt')\n","  w_load = load_tensor(Path + '/test.pt')\n","\n","  load_img = G.synthesis(w_load, noise_mode=noise_mode, force_fp32=False)\n","\n","  plot_generator_img(load_img, \"Reloaded image\")\n","\n","def save_img(seed,i):\n","\n","  device = 'cuda'\n","  truncation_psi = 0.7\n","  noise_mode = 'const'\n","\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=device)\n","\n","  #z = torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE), label, truncation_psi=truncation_psi).cpu().numpy().astype(np.float32)\n","\n","  img = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","\n","  #plot_generator_img(img, 'Image to Save')\n","\n","  save_generator_img(img, f'/content/drive/MyDrive/Bachelorarbeit/ausarbeitung/gfx/style_mixing/original_bilder/Bild_A{i}.png')\n","\n","\n","def load_img(Path):\n","\n","  device = 'cuda'\n","  truncation_psi = 0.7\n","  noise_mode = 'const'\n","\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=device)\n","\n","  w_load = load_tensor(Path)\n","  img_load = G.synthesis(w_load, noise_mode=noise_mode, force_fp32=False)\n","  plot_generator_img(img_load, \"Loaded Image\")\n","  save_generator_img(img_load, \"/content/drive/MyDrive/Bachelorarbeit/ausarbeitung/gfx/projection/images/see_reloaded.png\")\n","\n","load_img(\"/content/drive/MyDrive/Bachelorarbeit/ausarbeitung/gfx/projection/w_tensors/paul.pt\")"]},{"cell_type":"markdown","metadata":{"id":"Vz5Psw6RxsSO"},"source":["##Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8cOBoEhHy8un"},"outputs":[],"source":["IMG_PATH = \"/content/drive/My Drive/Bachelorarbeit/bachelorarbeit/projection/target_images/picasso.jpg\"\n","SAVE_PATH = \"/content/drive/My Drive/Bachelorarbeit/bachelorarbeit/projection/aligned_target_images/picasso.jpg\"\n","\n","preprocess_pipeline(IMG_PATH, True, SAVE_PATH)"]},{"cell_type":"markdown","metadata":{"id":"eV8h-Z3tfXip"},"source":["## Noise Mode Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WLYeTRT7faWQ"},"outputs":[],"source":["NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","DEVICE = 'cuda'\n","\n","def test_noise_modes(seed, count):\n","\n","  truncation_psi = 0.7\n","  noise_mode = 'none'\n","\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=DEVICE)\n","\n","  #zs = [torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE)  for i in range(count)]\n","\n","  #zs = torch.cat([z for z in zs]).to(DEVICE)\n","\n","  z = torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE)\n","  img_tensor = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","  save_generator_img(img_tensor,  f\"/content/drive/MyDrive/Bachelorarbeit/ausarbeitung/gfx/projection/none_noise{i}.png\")\n","\n","  plot_generator_img(img_tensor, \"\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RSI-tz5jgIq7"},"outputs":[],"source":["seeds = [12,14,16]\n","for i in range(len(seeds)):\n"," test_noise_modes(seeds[i], i)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wkuIyHXgHwy7"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"DkhD1BfQHx0Z"},"source":["##Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LkwWLmEhH4ZY"},"outputs":[],"source":["NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","DEVICE = 'cuda'\n","\n","def analysis_testing(seed):\n","\n","  truncation_psi = 0.7\n","  noise_mode = 'const'\n","\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=DEVICE)\n","\n","  noise_bufs = { name: buf for (name, buf) in G.synthesis.named_buffers() if 'noise_const' in name }\n","\n","  for buf in noise_bufs.values():\n","      buf[:] = torch.randn_like(buf)\n","      buf.requires_grad = True\n","\n","  print(noise_bufs)\n","\n","  \n","\n","  w_list = [G.mapping(gen_rand_z(G), label).cpu().numpy().astype(np.float32)  for i in range(2)]\n","  print(w_list)\n","  #w_list = w_list.cpu()\n","  #print(w_list)\n","  \n"," \n","  #w_tensor = torch.FloatTensor(w_list)\n","  #w_tensor = torch.stack(w_list)\n","  w_avg = np.mean(w_list, axis=0, keepdims=True)\n","  print(w_avg)\n","  #w_avg = torch.mean(w_tensor, 0)\n","\n","  w_std = (np.sum((w_list - w_avg) ** 2) / 10) ** 0.5\n","  print(w_std)\n","\n","\n","  #for tensor in w_tensor:\n","   # dist = torch.cdist(tensor, w_avg, p=2.0, compute_mode='use_mm_for_euclid_dist_if_necessary') \n","  #  sigma = torch.mean\n","  #  print(dist)\n","  \n"," \n","\n","analysis_testing(10)"]},{"cell_type":"markdown","metadata":{"id":"9mSMN6htsm2P"},"source":["##T-SNE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x37Z1Oqaspet"},"outputs":[],"source":["NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","\n","device = 'cuda'\n","truncation_psi = 0.7\n","noise_mode = 'const'\n","\n","G = init_generator(NETWORK_PKL)\n","label = torch.zeros([1, G.c_dim], device=device)\n","\n","size = 300\n","\n","z_list = [gen_rand_z(G).cpu().numpy().astype(np.float32) for i in range(size)]\n","np_zs = np.array(z_list)[:,0,:]\n","print(np_zs.shape)\n","\n","tsne = TSNE(n_components=3, verbose=1, random_state=123, perplexity=40)\n","embed = tsne.fit_transform(np_zs)\n","\n","x, y, z = list(zip(*embed))\n","\n","\n","fig = pylab.figure()\n","ax = fig.add_subplot(111, projection = '3d')\n","sc = ax.scatter(x,y,z)\n","\n","\n","#array = [\"Male\", \"Female\"]\n","\n","#color = np.random.randint(0,2,size)\n","#color = random.choices(array, k=size)\n","\n","#df = pd.DataFrame()\n","#df[\"y\"] = color\n","#df[\"comp-1\"] = embed[:,0]\n","#df[\"comp-2\"] = embed[:,1]\n","\n","\n","# hue=df.y.tolist()\n","#sns.scatterplot(x=\"comp-1\", y=\"comp-2\",hue=df.y.tolist(),\n","#                palette=sns.color_palette(\"hls\", 3),\n","#                data=df).set(title=\"Random Test data\") \n","\n","\n","#z_img_list = [G(z, label,  truncation_psi=truncation_psi, noise_mode=noise_mode).to(DEVICE) for z in z_list]\n","\n","#img = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ynX5i-S9DXj"},"outputs":[],"source":["NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","\n","device = 'cuda'\n","truncation_psi = 0.7\n","noise_mode = 'const'\n","\n","G = init_generator(NETWORK_PKL)\n","label = torch.zeros([1, G.c_dim], device=device)\n","\n","size = 300\n","\n","z_list = [gen_rand_z(G).cpu().numpy().astype(np.float32) for i in range(size)]\n","np_zs = np.array(z_list)[:,0,:]\n","print(np_zs.shape)\n","\n","tsne = TSNE(n_components=3, learning_rate='auto',\n","                  perplexity=20)\n","embed = tsne.fit_transform(np_zs)\n","\n","x, y, z = list(zip(*embed))\n","\n","# axes instance\n","fig = plt.figure(figsize=(6,6))\n","ax = Axes3D(fig, auto_add_to_figure=False)\n","fig.add_axes(ax)\n","\n","#color = np.random.randint(0,2,size)\n","#color = random.choices(array, k=size)\n","\n","\n","# get colormap from seaborn\n","cmap = ListedColormap(sns.color_palette(\"husl\", 256).as_hex())\n","\n","# plot\n","sc = ax.scatter(x, y, z, s=40, c='r', marker='o', alpha=1)\n","ax.set_xlabel('X Label')\n","ax.set_ylabel('Y Label')\n","ax.set_zlabel('Z Label')\n","\n","x, y, z = list(zip(*embed))\n","\n","\n","fig = pylab.figure()\n","ax = fig.add_subplot(111, projection = '3d')\n","sc = ax.scatter(x,y,z)\n","\n","\n","#array = [\"Male\", \"Female\"]\n","\n","#color = np.random.randint(0,2,size)\n","#color = random.choices(array, k=size)\n","\n","#df = pd.DataFrame()\n","#df[\"y\"] = color\n","#df[\"comp-1\"] = embed[:,0]\n","#df[\"comp-2\"] = embed[:,1]\n","\n","\n","# hue=df.y.tolist()\n","#sns.scatterplot(x=\"comp-1\", y=\"comp-2\",hue=df.y.tolist(),\n","#                palette=sns.color_palette(\"hls\", 3),\n","#                data=df).set(title=\"Random Test data\") \n","\n","\n","#z_img_list = [G(z, label,  truncation_psi=truncation_psi, noise_mode=noise_mode).to(DEVICE) for z in z_list]\n","\n","#img = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)"]},{"cell_type":"markdown","source":["## Truncation Psi"],"metadata":{"id":"vuRaLz3BU52_"}},{"cell_type":"code","source":["NETWORK_PKL = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl\"\n","\n","\n","def generate_img_psi(seed, psi):\n","  device = 'cuda'\n","  truncation_psi = 0.7\n","  noise_mode = 'const'\n","\n","  G = init_generator(NETWORK_PKL)\n","  label = torch.zeros([1, G.c_dim], device=device)\n","\n","  z = torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(DEVICE)\n","\n","  img = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n","  save_generator_img(img,  f'/content/drive/MyDrive/Bachelorarbeit/ausarbeitung/gfx/style_mixing/original_bilder/BildB{seed}.png')"],"metadata":{"id":"WAVALxnWVAie"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seeds = [1500,1501,1502,1503,1504]\n","for seed in seeds:\n","  generate_img_psi(seed, 0.7)"],"metadata":{"id":"pHWebWixVi1Z"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"toc_visible":true,"provenance":[],"authorship_tag":"ABX9TyMYkp8w/JP7FwRXgJHQ4C1j"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}